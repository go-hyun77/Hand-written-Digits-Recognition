{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>CPSC 585-01 Spring 2023 Introductory Project, Gordon Huynh</h1>**\n",
        " <li>This introductory project focuses on building skills with Python and some of the libraries and tools needed to work with neural network models. </li>\n",
        "\n"
      ],
      "metadata": {
        "id": "6K75es2umXZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras.layers import Input, Dense, Dropout\n"
      ],
      "metadata": {
        "id": "dXhuIy9_U4mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Importing the ScikitLearn Dataset</h1>**\n",
        "<li>The <a href=\"https://scikit-learn.org/stable/datasets/toy_dataset.html#digits-dataset\">Optical recognition of handwritten digits dataset</a>, included with the scikit-learn library consists of 1797 8✕8 grayscale images of hand-written digits sorted into classes 0 to 9, in which the values of the arrays vary from 0 to 16.</li>"
      ],
      "metadata": {
        "id": "S_WAqCwsGyrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the handwritten digits dataset\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "PPTmdCLr2Y3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Verifying the Dataset Attributes</h1>**\n",
        "<li>We check the shape of the data/images below with:\n",
        "\n",
        "```\n",
        "digits.data.shape\n",
        "```\n",
        "\n",
        "```\n",
        "digits.images.shape\n",
        "```\n",
        "\n",
        "\n",
        "</li>"
      ],
      "metadata": {
        "id": "bqpmWG1oJBQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the digits from data set\n",
        "digits = load_digits()"
      ],
      "metadata": {
        "id": "FjQrSl7s2aIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking number of instances/attributes\n",
        "print(digits.data.shape)"
      ],
      "metadata": {
        "id": "HGyGa7yz2eDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3db80e-9c7e-4733-fc4b-77922268ec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of digits data, dataset has 1791 images in total, and each of these are in an 8x8 array of grayscale pixels (64 total pixels)\n",
        "print(\"Digits Data Shape:\", digits.images.shape)"
      ],
      "metadata": {
        "id": "hWtVqCc4q7pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35abeb44-0f85-43fa-ad64-e816c1ad5a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits Data Shape: (1797, 8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Visualizing the Dataset Arrays</h1>**\n",
        "<li>A test print of the first array index yields the aforementioned 8✕8 array of values ranging from 0 to 16. </li>\n",
        "\n",
        "> <li>Plotting the image yields a grayscale 8✕8 graph in which we can see the array values functioning as a spectrum of grayscale intensity, with a value of \"0\" representing a completely white cell, and a \"16\" representing a completely black cell. </li><br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lS-VMaCMJ2OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test print an instance of digits array, values of the array range from 0 to 16\n",
        "print(digits.images[0])"
      ],
      "metadata": {
        "id": "clkDPpS2rOSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c98b05-0c22-4b97-8943-49c3412fca3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
            " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
            " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
            " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
            " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
            " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
            " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
            " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test print the image of the above instance\n",
        "plt.imshow(digits.images[0],cmap='binary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EFNivIfEruhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "16a568c5-284d-45fb-b01d-2a01fc12a2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKpElEQVR4nO3dX4hc9RnG8efpqrRWo7EJRbKhm4AEpFATl4CkCI1siVW0F1USUKgUvKmitGC0d73TG7EXRZCoFUyVbFQQsVpBpRVa624SW5PVksSUbNAmoRH/XDRE317sCURZ3TNnzr99+/3A4s7usL93SL6emdmT83NECEAeX+t6AAD1ImogGaIGkiFqIBmiBpI5q4kfumzZshgbG2viR3fqxIkTra43Ozvb2lpLlixpba3R0dHW1hoZGWltrTYdOnRIx48f93zfayTqsbExTU1NNfGjOzU5Odnqelu3bm1trYmJidbWuvfee1tba+nSpa2t1abx8fEv/R5Pv4FkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZEpFbXuT7Xds77d9d9NDAahuwahtj0j6raSrJV0qaYvtS5seDEA1ZY7U6yXtj4iDEXFS0pOSrm92LABVlYl6haTDZ9yeLb72ObZvtT1le+rYsWN1zQdgQLW9URYRD0XEeESML1++vK4fC2BAZaI+ImnlGbdHi68B6KEyUb8h6RLbq2yfI2mzpGebHQtAVQteJCEiTtm+TdKLkkYkPRIRexufDEAlpa58EhHPS3q+4VkA1IAzyoBkiBpIhqiBZIgaSIaogWSIGkiGqIFkGtmhI6s2d8yQpHfffbe1tdrcUuiiiy5qba0dO3a0tpYk3XDDDa2uNx+O1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJFNmh45HbB+1/VYbAwEYTpkj9e8kbWp4DgA1WTDqiPiTpP+0MAuAGtT2mpptd4B+YNsdIBne/QaSIWogmTK/0npC0l8krbE9a/tnzY8FoKoye2ltaWMQAPXg6TeQDFEDyRA1kAxRA8kQNZAMUQPJEDWQzKLfdmd6erq1tdrcBkeSDhw40Npaq1evbm2tiYmJ1tZq8++HxLY7ABpA1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMmWuUbbS9iu299nea/uONgYDUE2Zc79PSfplROyyfb6kadsvRcS+hmcDUEGZbXfei4hdxecfSZqRtKLpwQBUM9BrattjktZKen2e77HtDtADpaO2fZ6kpyTdGREffvH7bLsD9EOpqG2frbmgt0fE082OBGAYZd79tqSHJc1ExP3NjwRgGGWO1Bsk3Sxpo+09xcePGp4LQEVltt15TZJbmAVADTijDEiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkFv1eWidOnGhtrXXr1rW2ltTu/lZtuvzyy7seITWO1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMmUuPPh123+z/Wax7c6v2xgMQDVlThP9r6SNEfFxcang12z/ISL+2vBsACooc+HBkPRxcfPs4iOaHApAdWUv5j9ie4+ko5Jeigi23QF6qlTUEfFpRFwmaVTSetvfnec+bLsD9MBA735HxAeSXpG0qZFpAAytzLvfy21fWHz+DUkTkt5ueC4AFZV59/tiSY/ZHtHc/wR2RMRzzY4FoKoy737/XXN7UgNYBDijDEiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFk2HZnABMTE62tlVmbf2ZLly5tba2+4EgNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAypaMuLui/2zYXHQR6bJAj9R2SZpoaBEA9ym67MyrpGknbmh0HwLDKHqkfkHSXpM++7A7spQX0Q5kdOq6VdDQipr/qfuylBfRDmSP1BknX2T4k6UlJG20/3uhUACpbMOqIuCciRiNiTNJmSS9HxE2NTwagEn5PDSQz0OWMIuJVSa82MgmAWnCkBpIhaiAZogaSIWogGaIGkiFqIBmiBpJZ9NvutLmtyvT0V57+vqi1uRXO1NRUa2vdeOONra3VFxypgWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIptRposWVRD+S9KmkUxEx3uRQAKob5NzvH0TE8cYmAVALnn4DyZSNOiT90fa07VvnuwPb7gD9UDbq70fEOklXS/q57Su/eAe23QH6oVTUEXGk+O9RSc9IWt/kUACqK7NB3jdtn3/6c0k/lPRW04MBqKbMu9/flvSM7dP3/31EvNDoVAAqWzDqiDgo6XstzAKgBvxKC0iGqIFkiBpIhqiBZIgaSIaogWSIGkhm0W+7s3r16tbWanO7GEmanJxMuVabtm7d2vUIreNIDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMqWitn2h7Z2237Y9Y/uKpgcDUE3Zc79/I+mFiPiJ7XMkndvgTACGsGDUti+QdKWkn0pSRJyUdLLZsQBUVebp9ypJxyQ9anu37W3F9b8/h213gH4oE/VZktZJejAi1kr6RNLdX7wT2+4A/VAm6llJsxHxenF7p+YiB9BDC0YdEe9LOmx7TfGlqyTta3QqAJWVfff7dknbi3e+D0q6pbmRAAyjVNQRsUfSeLOjAKgDZ5QByRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAx7aQ3gvvvua20tqd19oMbH2zu3aHp6urW1/h9xpAaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGklkwattrbO854+ND23e2MBuAChY8TTQi3pF0mSTZHpF0RNIzzY4FoKpBn35fJelARPyriWEADG/QqDdLemK+b7DtDtAPpaMurvl9naTJ+b7PtjtAPwxypL5a0q6I+HdTwwAY3iBRb9GXPPUG0B+loi62rp2Q9HSz4wAYVtltdz6R9K2GZwFQA84oA5IhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZR0T9P9Q+JmnQf565TNLx2ofph6yPjcfVne9ExLz/cqqRqKuwPRUR7W3o1KKsj43H1U88/QaSIWogmT5F/VDXAzQo62PjcfVQb15TA6hHn47UAGpA1EAyvYja9ibb79jeb/vuruepg+2Vtl+xvc/2Xtt3dD1TnWyP2N5t+7muZ6mT7Qtt77T9tu0Z21d0PdOgOn9NXWwQ8E/NXS5pVtIbkrZExL5OBxuS7YslXRwRu2yfL2la0o8X++M6zfYvJI1LWhIR13Y9T11sPybpzxGxrbiC7rkR8UHHYw2kD0fq9ZL2R8TBiDgp6UlJ13c809Ai4r2I2FV8/pGkGUkrup2qHrZHJV0jaVvXs9TJ9gWSrpT0sCRFxMnFFrTUj6hXSDp8xu1ZJfnLf5rtMUlrJb3e8Sh1eUDSXZI+63iOuq2SdEzSo8VLi23FRTcXlT5EnZrt8yQ9JenOiPiw63mGZftaSUcjYrrrWRpwlqR1kh6MiLWSPpG06N7j6UPURyStPOP2aPG1Rc/22ZoLentEZLm88gZJ19k+pLmXShttP97tSLWZlTQbEaefUe3UXOSLSh+ifkPSJbZXFW9MbJb0bMczDc22NffabCYi7u96nrpExD0RMRoRY5r7s3o5Im7qeKxaRMT7kg7bXlN86SpJi+6NzVLX/W5SRJyyfZukFyWNSHokIvZ2PFYdNki6WdI/bO8pvvariHi+u5FQwu2SthcHmIOSbul4noF1/istAPXqw9NvADUiaiAZogaSIWogGaIGkiFqIBmiBpL5H9Sir9XgxKzrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize digits\n",
        "plt.figure()\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(\"Train: %i\" % label)\n"
      ],
      "metadata": {
        "id": "r2HlZcC62bEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "b21ff927-a599-4ba4-f7fc-b2a826d3534d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAABNCAYAAACi7r7XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAANzklEQVR4nO3de4wdZRnH8e9DS+Qm3a0oKMa2EBCikZaSYLy11daKl7RCIChiS4JtMCCgxjYG04IaW0NkiUStCWmrJMY2QUrwCtJWS+KlFRZDEAK0NRAabm2h3FR4/GOmum7eZzlzdmbO8c3vk2x69uk5M/PsvDPnPTPvc15zd0RERERydkivN0BERESkaerwiIiISPbU4REREZHsqcMjIiIi2VOHR0RERLKnDo+IiIhkr/UOj5n90swWtb3etuSeHyjHXOSeY+75gXLMQe75QR/l6O6v+QMcGPHzKvDiiN8v6GQZTfwAk4GfAc8Du4FPd7mcfs3vUmA78DKwbpzL6rscgdcBN5b77jngHuCsnHIst+sm4HHgWeBB4OLcchyxfScBLwE35ZQfsKXM6+C2PJDjPgTOB+4vz6kPA+/PKcdR23UAeAX4bkb5TQV+AewF9gA3ABMz24enAncC+4GHgE92/NouVrYLmBv8X1d/2HEk/hPgp8BRwPvKP8A7xrnMfsrvbGAh8H3G2eHpxxyBI4GV5UF6CPBxio7P1FxyLNf3DuB15eNTyhPRzJxyHLHe3wC/p8sOT7/mR9Hh6bqj+n+S4zyKDx/vLo/H44Hjc8px1LqPKt+8P5BLfmVnZx1wGHAc8FfgC7nsQ2AixYfGLwITgA9SdM5P7uT147qlZWazzexRM1tmZnuAtWY2aGa3mdmTZra3fPzWEa/ZYmYXl48Xm9k2M7u2fO5OMzurw3UfCZwDfM3dD7j7NuBW4MLx5NQv+QG4+83ufgvwdF05jdbLHN39eXdf6e673P1Vd78N2AnMzCXHMs/73P3lg7+WPyfmlGO5jPOBfcBv68ytXHbP82taH+R4NXCNu/+hPB4fc/fHMstxpHOAJyg66LXog/ymARvc/SV33wP8iuIDV216nOMpwFuA69z9FXe/E7iLDt/36xjDcxzFraUpwJJymWvL399GcRnshjFefybwAHAM8G3gRjMzADNbbma3Ba87GfiXuz84IjZMzTuX3uXXpr7I0cyOpdiv93WXxph6mqOZfc/MXgD+RnF76xfjyiatZzma2dHANRSfvJrS63b6LTN7yszuMrPZ48hjLD3J0cwmAGcAbzSzh8o3tBvM7PB60vofvd6PBy0CfuTlpYMa9TK/IeB8MzvCzI4HzqLo9NStX/YhgAHv7OiZ47m0BcwG/gEcNsbzpwN7R/y+hfLSMLAYeGjE/x1B8en3uA624/3AnlGxzwFb6rp018v8Rq3jGzR0S6uPcjwUuANYk3GOEyhuvV4FHJpTjsD1wLLy8UpqvqXVB/mdCbyeYtzZIopbryfmkiPFp2anGDP4Zoo3oruAb+aS46h1TKEYvzMtp/woxrfsAP5Vvm4dYLnkSPE+8QjwlfLxh8tt+XUnedRxhedJd3/p4C9lz3KNme02s2eB3wED5SeIlD0HH7j7C+XDozpY7wHg6FGxoylORHXqVX5t6mmOZnYI8GOKhntp5a3vTM/3oxeXYLcBbwUuqbb5HelJjmY2HZgLXNf1lnemZ/vQ3f/o7s+5+8vuvp6iM/DR7tIYU69yfLH897vu/ri7PwV8h7xyHOlCYJu776z4uk706jg8hOJqzs0U4yOPAQaB1d2lMaae5Oju/6QY1/qxchlfAjYAj3ay0XV0eEZfDvwS8HbgTHc/GvhAGbca1jXSg8BEMztpROw06r8d0qv82tSzHMvLmDcCxwLnlA26Cf20HydS8xieUq9ynE0x8PzvVtzT/zJwjpn9peb19NM+9IbW05Mc3X0vxZvGyPXXfasnWm4v9uNngfUNLbtX+U2muJ10Q9kxf5riNlMTndae7UN3v9fdZ7n7G9x9PnAC8KdOXtvE9/C8nuLTwj4zmwysaGAduPvzFD3Za8zsSDN7L7CA4kpBk1rJD8DMJprZYRS3QiaY2WFmNrGp9Y3QWo4UFWinAp9w9xdf68k1aiVHM3uTmZ1vZkeZ2QQzmw98igYG9ia0tR9/SNGBm17+/AD4OTC/ofUd1NY+HDCz+QePPzO7gOKE3sTYiNHaPBbXApeVbXYQuBJoY4xhmzliZu+hqEDb2OR6RmjrPfEpiqKPS8p2OkBx+/XeJtY3Spvvi+8qj8UjzOzLFLdg13Xy2iY6PEPA4cBTwB8Yx0nBzL5qZr8c4ymfL9f1BEWJ+iXu3sSA15GGaC+/qyga0XLgM+Xjq7pdXwVDtJCjmU0BllK8Se4xswPlzwXdrq+CIdrZj05x++pRiu/GuBa4wt1v7XZ9FQzRQo7u/oK77zn4Q3G7+SV3f7Lb9XVoiHb24aEU4+ieLNd1GbDQ/7dgoilDtHe++TrwZ4qr5/cDdwPf7HZ9FQzRXo5QdAJudve6hz9Ehmgvv7OBj1C01YeAf1J0XJs2RHs5XkhR+PEE8CFgnv+3CnbsZZcDgURERESypbm0REREJHvq8IiIiEj21OERERGR7KnDIyIiItlTh0dERESy91rf6VKphGvjxvTXGixbtiwZnzdvXjK+atWqZHxwcLDK5kBnX3pUS5na7Nmzk/F9+/Yl41dffXUyvmDBgqqrbi3HLVu2JOMLFy5MxqdPn15pOWN4rRwr5bd6dfqLR5cvX56MT5s2LRnfsWNHMt7P7TRqj4sXL07Gb7nlljpWCw3kGB1zU6dOTcbXrVtXZfHd6NvzzT333FPHaqHmY3FoaCgZj/KI2uPw8HAyPmnSpGR8165dyfjAwEDt+/CKK65IxqNcomMxWs7AwECVzYEG2mn0HhDtxy7eA6pK5qgrPCIiIpI9dXhEREQke+rwiIiISPbU4REREZHs1ToRZTQ4eefOncn43r17k/HJkycn4xs2bEjGzz333A62rlnRwLGtW7cm45s3b07Guxi0XLtogOOcOXOS8aoDA9sSDUKO2tGaNWuS8aVLlybj0aDluXPndrB1vREN3I0GmPezqH1Fx9z69enJsadMmVJp+W3atGlTMh7luGJFo/NutiY6n0aDnKsOfu5ioG/Xqg4Yj47RaKBvCwOA/yM6JqJ2GjFLj5s+7bTTkvG6Bt3rCo+IiIhkTx0eERERyZ46PCIiIpI9dXhEREQke+rwiIiISPa6qtKKqlOiaqyHH344GT/hhBOS8WjKiWi9bVZpRaPFq46U7+eqmOgrz6MR9NHXikfTZ7RlyZIlyXhUTThz5sxkPJpaop+rsaLqlKgCJPra+qqVStG0Dk2IKm12796djEfVhFWnaWizwqdq1VV0LParqN1FVq5cmYxH7bTNCqZIdK6vOgVK1O6iHKN2PR7RMRGZNWtWMh7l3vT+0hUeERERyZ46PCIiIpI9dXhEREQke+rwiIiISPbU4REREZHsdVWlFc2BdfrppyfjUTVWJKqWaVM0N0tUJbB///5Ky29iBH1dosqJaGR99PxezwsWtbtHHnkkGY+qDKNqrOg4GBwc7GDrmhVVekTVLIsXL07Go30bVYxEx0cTovY4PDycjEfHaFRF02Y1ViSqiokqJvu1+rOueaCi83IkqjiN2nsTonXNmDEjGY+O0ag9tlkZWXVd0d8/qiasWgVWla7wiIiISPbU4REREZHsqcMjIiIi2VOHR0RERLKnDo+IiIhkr9YqrWgOrLqW32b1S1SdEo24r7ptTY9GH882RJUQ0Yj7SFQp1GtR9dYzzzyTjEdVWlH8jjvuSMabaL+bNm1Kxq+88spkfNGiRZWWf/311yfja9eurbScJkTtMar8iebBi/5WkarzP41HdIxG1TLRsRtVxbRV4ROtp665CaO20A/VsFXP9Vu3bk3GoyrSfpi/LqoajM55l19+eTIetYeocq1q7rrCIyIiItlTh0dERESypw6PiIiIZE8dHhEREcmeOjwiIiKSva6qtKKR1zt27Ki0nKgaa/v27cn4eeedV2n5/Swajd7mXDjRvEdRZU4kqpDoh7mIqojadVR1tXTp0mR89erVyfiqVau627AxTJo0qVJ8/fr1yXjUHiNR1U8/qKsyJ6oMaVNUhRJV8kQVQVEl2t13352M130eivKIzh1mVun5/VCNFR1Dc+bMScZXrFiRjEftLjrmor9Jm9VbUe51vc9FlZFVK4d1hUdERESypw6PiIiIZE8dHhEREcmeOjwiIiKSPXV4REREJHtdVWlFcxFF1VUbN26sFI8sW7as0vNlbNG8YNE8NsPDw8l4VD2wYMGCZPyiiy6q9Py6LV++PBmP5saKqglvv/32ZLzNasKoOiWq1omqJqLlRHNv9UMFXjSPWFShFlUlRvqhEi06RqOqq6gyJ6r8iapc2qoWjapvon04a9asBrdmfKK/fZRLlHu0r2bMmJGMR3MWVm3vTYjaUZR7lEvVaqyIrvCIiIhI9tThERERkeypwyMiIiLZU4dHREREsqcOj4iIiGSv1iqtaA6hqLrqjDPOSMarzsnVpqg6JaowiipJokqoqCqjCdEI+qrzokTVAFHuUTVDW1Va0ZxZS5YsqbScqBprzZo1lbepLVH73b9/fzLeZnusavPmzcl41bngokq0fpifKfr7R5U8UZVLlEuvK9Gi82A051s/VAdGom2L/vbReSiq6orOj1HFU5uibYjeM6Iq0qg91FU1qCs8IiIikj11eERERCR76vCIiIhI9tThERERkeypwyMiIiLZM3fv9TaIiIiINEpXeERERCR76vCIiIhI9tThERERkeypwyMiIiLZU4dHREREsqcOj4iIiGTv36WCaSWi7f2aAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Task 1.</h1>**\n"
      ],
      "metadata": {
        "id": "5Te9NnqNPhbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Transforming the Dataset</h1>**\n",
        "<li>The input layer of the neural network requires a 1-dimensional array as input.</li>\n",
        "\n",
        "> <li>Currently, the data from ScikitLearn Dataset is by default in 2-dimensional arrays (8, 8). Thus, we flatten the images from 2-dimensional arrays (8, 8) into 1-dimensional arrays of shape (64, ), where (n_samples, n_features) such that n_samples is the number of images and n_features is the total number of pixels in each image.</li>\n",
        "\n"
      ],
      "metadata": {
        "id": "svKpIUBlNGgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten images \n",
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))"
      ],
      "metadata": {
        "id": "4HCApT4l2iUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Splitting the Dataset</h1>**\n",
        "<li>The dataset is split in two, half of which will be used to train the network, and the other half will be used to test the network.</li><br>"
      ],
      "metadata": {
        "id": "yXv5LHKVPodx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into 50% training and 50% testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data, digits.target, test_size = 0.5)"
      ],
      "metadata": {
        "id": "KQ4oVvsO-mRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify test/train data split correctly\n",
        "print(\"Train Data Shape:\", X_train.shape)\n",
        "print(\"Test Data Shape:\", X_test.shape)\n",
        "print(\"Target Train Data Shape:\", Y_train.shape)\n",
        "print(\"Target Test Data Shape:\", Y_test.shape)"
      ],
      "metadata": {
        "id": "wNpVZVuVBx4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5986986-b6ad-434b-a1a4-77340aaf3d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: (898, 64)\n",
            "Test Data Shape: (899, 64)\n",
            "Target Train Data Shape: (898,)\n",
            "Target Test Data Shape: (899,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data to between 0 and 1 to help with the training\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# print input shape ready for training \n",
        "print(\"Train Matrix Shape\", X_train.shape) \n",
        "print(\"Test Matrix Shape\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3OVsTiinqnO",
        "outputId": "a33cef7c-b914-4d61-9492-6cbd27b5b113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Matrix Shape (898, 64)\n",
            "Test Matrix Shape (899, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>One Hot-Encoding Output</h1>**\n",
        "<li>Currently, the output neurons can only give an output in the range of 0 to 1. Thus, we convert discrete output values to categorical values so that output values can be represented as a vector of zero and one with the length equal to the number of classes (this case being 10, as there are 10 different digits to recognize, 0, 1, 2, 3, 4, ... 9.)</li>"
      ],
      "metadata": {
        "id": "FGbI1sGNQuL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hot encoding training with keras np_utils\n",
        "n_classes = 10\n",
        "\n",
        "print(\"Shape before one-hot encoding: \", Y_train.shape)\n",
        "Y_train = np_utils.to_categorical(Y_train, n_classes) \n",
        "Y_test = np_utils.to_categorical(Y_test, n_classes) \n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLWtS1C8-dHz",
        "outputId": "9c0391b3-13c0-47a1-a47a-90714c4c8e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before one-hot encoding:  (898,)\n",
            "Shape after one-hot encoding:  (898, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Defining Neural Network Model Architecture</h1>**\n",
        "<li><code>tf.keras.Sequential</code> groups a linear stack of layers into a <code>tf.keras.Model</code>.</li><br>\n",
        "\n",
        "<li><code>tf.keras.layers.Dense</code> is a regular densely-connected Neural Network Layer.</li><br>\n",
        "\n",
        "<li><code>tf.keras.layers.Dropout</code>: applies Dropout to the input, randomly setting input units to 0 at a specified frequency (in this case 0.2) during each training step in order to avoid overfitting.</li><br>\n"
      ],
      "metadata": {
        "id": "CoiohotvTmD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Function Call Parameters</h1>**\n",
        "\n",
        "<li>Listed below are the parameters for functions in which we add layers to the model.</li>\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense(units, input_shape, name))\n",
        "```\n",
        "\n",
        "\n",
        "<li><code>units</code>: A positive integer, dimensionality of the output \n",
        "space. <code>64</code> is specified to be the dimensionality of the layer in the section below, giving us a layer of 64 neurons. .</li><br>\n",
        "\n",
        "<li><code>input_shape</code>: Specifies the input shape, in this case being <code>X_train.shape[1]</code> in the section below.</li><br>\n",
        "\n",
        "<li><code>name</code>: Name of the layer, in this case being <code>name=\"input_layer\"</code> in the section below.</li><br>\n",
        "\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Activation(activation)\n",
        "```\n",
        "\n",
        "<li><code>activation</code>: Activation function to use. The activations used in this project are listed as follows.</li><br>\n",
        "\n",
        "<li><code>relu</code>: Computes the rectified linear unit: <code>max(features, 0)</code>.</li><br>\n",
        "  \n",
        "<li><code>softmax</code>: Computes softmax activations, used for multi-class predictions. The sum of all outputs generated by softmax is 1. This function performs the equivalent of <code>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)</code>.</li>\n",
        "\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dropout(rate))\n",
        "```\n",
        "<li><code>rate</code>: Value of type float between 0 and 1, the value represents the fraction of the input units to drop.</li><br>"
      ],
      "metadata": {
        "id": "1g-78785WxM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, input_shape=(X_train.shape[1],), name=\"input_layer\"))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(64, name=\"hidden_layer\"))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(10, name=\"output_layer\"))\n",
        "model.add(tf.keras.layers.Activation('softmax'))"
      ],
      "metadata": {
        "id": "cCh_orJsNsGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Model Summary and Training</h1>**\n",
        "\n",
        "<li>As we can see, the model is defined having an input layer of 64 neurons, a hidden layer of 64 neurons, and an output layer of having 10 neurons.</li><br>"
      ],
      "metadata": {
        "id": "ARS_2wgwcJM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print model attributes to check for layers/parameters\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "MdLdQQwVNiU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0336c274-aafa-4eaf-d299-8d80d094f6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (Dense)         (None, 64)                4160      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " hidden_layer (Dense)        (None, 64)                4160      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                650       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,970\n",
            "Trainable params: 8,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Compiling the Model</h1>**\n",
        "\n",
        "<li><code>model.compile()</code> configures the model for training.</li><br>\n",
        "\n",
        "<li><code>optimizer</code>: Built-in optimizer classes.</li>\n",
        "\n",
        "> <li><code>tf.keras.optimizers.Adam</code> implements the Adam algorithm, a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.</li>\n",
        "\n",
        "<li><code>loss</code>: Built-in optimizer loss functions.\n",
        "\n",
        "> <li><code>tf.keras.losses.CategoricalCrossentropy</code> computes the crossentropy loss between the labels and predictions.</li>\n",
        "\n",
        "<li><code>metrics</code>: List of metrics to be evaluated by the model during training and testing.\n",
        "\n",
        "> <li><code>tf.keras.metrics.Accuracy</code> calculates how often predictions match labels.</li>"
      ],
      "metadata": {
        "id": "kW05AGWEeORe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pxcG79s_Nnh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Training the Model</h1>**\n",
        "<li>Listed below are the parameters for the function in which we train model.</li>\n",
        "\n",
        "```\n",
        "model.fit(x, y, epochs, batch_size, validation_data)\n",
        "```\n",
        "\n",
        "<li><code>x</code>: The input data; in this case, <code>X_train</code> is the input data below.</li><br>\n",
        "\n",
        "<li><code>y</code>: The target data; in this case, <code>Y_train</code> is the target data below.</li><br>\n",
        "\n",
        "<li><code>epochs</code>: The number of epochs (an iteration over the entire x and y data provided) to train the model, in this case being <code> 400</code> below.</li><br>\n",
        "\n",
        "<li><code>batch_size</code>: The number of samples per gradient update, in this case, being <code>64</code> below.</li><br>\n",
        "\n",
        "<li><code>validation_data</code>: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.</li><br>\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Cw9hq5L7nQzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "history = model.fit(X_train, Y_train, epochs=400, batch_size=64, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "id": "KmyypwPoNpVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6663bce-45f9-4a6f-e827-03bb501a2e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "15/15 [==============================] - 2s 27ms/step - loss: 2.2981 - accuracy: 0.1314 - val_loss: 2.2894 - val_accuracy: 0.1935\n",
            "Epoch 2/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.2849 - accuracy: 0.2004 - val_loss: 2.2752 - val_accuracy: 0.2269\n",
            "Epoch 3/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2674 - accuracy: 0.2962 - val_loss: 2.2544 - val_accuracy: 0.4071\n",
            "Epoch 4/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.2424 - accuracy: 0.3664 - val_loss: 2.2261 - val_accuracy: 0.3882\n",
            "Epoch 5/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.2082 - accuracy: 0.4154 - val_loss: 2.1869 - val_accuracy: 0.4549\n",
            "Epoch 6/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.1675 - accuracy: 0.4287 - val_loss: 2.1378 - val_accuracy: 0.5751\n",
            "Epoch 7/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.1129 - accuracy: 0.5267 - val_loss: 2.0683 - val_accuracy: 0.6363\n",
            "Epoch 8/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.0364 - accuracy: 0.5679 - val_loss: 1.9845 - val_accuracy: 0.6652\n",
            "Epoch 9/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.9557 - accuracy: 0.5724 - val_loss: 1.8869 - val_accuracy: 0.6952\n",
            "Epoch 10/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.8532 - accuracy: 0.6180 - val_loss: 1.7857 - val_accuracy: 0.7041\n",
            "Epoch 11/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.7340 - accuracy: 0.6470 - val_loss: 1.6714 - val_accuracy: 0.7041\n",
            "Epoch 12/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.6428 - accuracy: 0.6414 - val_loss: 1.5497 - val_accuracy: 0.7620\n",
            "Epoch 13/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.5177 - accuracy: 0.6904 - val_loss: 1.4369 - val_accuracy: 0.7720\n",
            "Epoch 14/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4240 - accuracy: 0.6759 - val_loss: 1.3315 - val_accuracy: 0.7620\n",
            "Epoch 15/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.3124 - accuracy: 0.6949 - val_loss: 1.2300 - val_accuracy: 0.7564\n",
            "Epoch 16/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.2334 - accuracy: 0.7004 - val_loss: 1.1334 - val_accuracy: 0.7753\n",
            "Epoch 17/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1457 - accuracy: 0.7105 - val_loss: 1.0578 - val_accuracy: 0.7809\n",
            "Epoch 18/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.0944 - accuracy: 0.7004 - val_loss: 1.0057 - val_accuracy: 0.7697\n",
            "Epoch 19/400\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 1.0310 - accuracy: 0.7294 - val_loss: 0.9324 - val_accuracy: 0.8154\n",
            "Epoch 20/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.9858 - accuracy: 0.7472 - val_loss: 0.8831 - val_accuracy: 0.8331\n",
            "Epoch 21/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.9340 - accuracy: 0.7695 - val_loss: 0.8317 - val_accuracy: 0.8276\n",
            "Epoch 22/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8975 - accuracy: 0.7539 - val_loss: 0.7903 - val_accuracy: 0.8387\n",
            "Epoch 23/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.8382 - accuracy: 0.7739 - val_loss: 0.7592 - val_accuracy: 0.8265\n",
            "Epoch 24/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8228 - accuracy: 0.7795 - val_loss: 0.7244 - val_accuracy: 0.8420\n",
            "Epoch 25/400\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.8001 - accuracy: 0.7762 - val_loss: 0.6978 - val_accuracy: 0.8309\n",
            "Epoch 26/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7688 - accuracy: 0.7929 - val_loss: 0.6661 - val_accuracy: 0.8443\n",
            "Epoch 27/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7412 - accuracy: 0.7851 - val_loss: 0.6452 - val_accuracy: 0.8454\n",
            "Epoch 28/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6980 - accuracy: 0.8018 - val_loss: 0.6188 - val_accuracy: 0.8476\n",
            "Epoch 29/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6851 - accuracy: 0.7973 - val_loss: 0.6122 - val_accuracy: 0.8476\n",
            "Epoch 30/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.7784 - val_loss: 0.5964 - val_accuracy: 0.8699\n",
            "Epoch 31/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6529 - accuracy: 0.8118 - val_loss: 0.5740 - val_accuracy: 0.8532\n",
            "Epoch 32/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6333 - accuracy: 0.8241 - val_loss: 0.5567 - val_accuracy: 0.8565\n",
            "Epoch 33/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6616 - accuracy: 0.7873 - val_loss: 0.5439 - val_accuracy: 0.8587\n",
            "Epoch 34/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6165 - accuracy: 0.8252 - val_loss: 0.5300 - val_accuracy: 0.8699\n",
            "Epoch 35/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6068 - accuracy: 0.8296 - val_loss: 0.5190 - val_accuracy: 0.8754\n",
            "Epoch 36/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5727 - accuracy: 0.8241 - val_loss: 0.5030 - val_accuracy: 0.8843\n",
            "Epoch 37/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5713 - accuracy: 0.8363 - val_loss: 0.4905 - val_accuracy: 0.8776\n",
            "Epoch 38/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5593 - accuracy: 0.8441 - val_loss: 0.4803 - val_accuracy: 0.8732\n",
            "Epoch 39/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5370 - accuracy: 0.8430 - val_loss: 0.4720 - val_accuracy: 0.8710\n",
            "Epoch 40/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5303 - accuracy: 0.8352 - val_loss: 0.4616 - val_accuracy: 0.8943\n",
            "Epoch 41/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5381 - accuracy: 0.8196 - val_loss: 0.4537 - val_accuracy: 0.8921\n",
            "Epoch 42/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5285 - accuracy: 0.8430 - val_loss: 0.4430 - val_accuracy: 0.8954\n",
            "Epoch 43/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5018 - accuracy: 0.8508 - val_loss: 0.4336 - val_accuracy: 0.8977\n",
            "Epoch 44/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4895 - accuracy: 0.8597 - val_loss: 0.4250 - val_accuracy: 0.8977\n",
            "Epoch 45/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4954 - accuracy: 0.8486 - val_loss: 0.4190 - val_accuracy: 0.9021\n",
            "Epoch 46/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4783 - accuracy: 0.8586 - val_loss: 0.4176 - val_accuracy: 0.8943\n",
            "Epoch 47/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4762 - accuracy: 0.8552 - val_loss: 0.4040 - val_accuracy: 0.8977\n",
            "Epoch 48/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4427 - accuracy: 0.8708 - val_loss: 0.3978 - val_accuracy: 0.8988\n",
            "Epoch 49/400\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4442 - accuracy: 0.8764 - val_loss: 0.4063 - val_accuracy: 0.8854\n",
            "Epoch 50/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4591 - accuracy: 0.8530 - val_loss: 0.3917 - val_accuracy: 0.8988\n",
            "Epoch 51/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4608 - accuracy: 0.8508 - val_loss: 0.3802 - val_accuracy: 0.9055\n",
            "Epoch 52/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4358 - accuracy: 0.8586 - val_loss: 0.3724 - val_accuracy: 0.9088\n",
            "Epoch 53/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4233 - accuracy: 0.8742 - val_loss: 0.3754 - val_accuracy: 0.8966\n",
            "Epoch 54/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4183 - accuracy: 0.8586 - val_loss: 0.3807 - val_accuracy: 0.8954\n",
            "Epoch 55/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4114 - accuracy: 0.8697 - val_loss: 0.3580 - val_accuracy: 0.9088\n",
            "Epoch 56/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4153 - accuracy: 0.8808 - val_loss: 0.3778 - val_accuracy: 0.8932\n",
            "Epoch 57/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.4206 - accuracy: 0.8664 - val_loss: 0.3561 - val_accuracy: 0.9110\n",
            "Epoch 58/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3880 - accuracy: 0.8842 - val_loss: 0.3439 - val_accuracy: 0.9121\n",
            "Epoch 59/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3922 - accuracy: 0.8808 - val_loss: 0.3420 - val_accuracy: 0.9110\n",
            "Epoch 60/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3694 - accuracy: 0.8931 - val_loss: 0.3417 - val_accuracy: 0.9177\n",
            "Epoch 61/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3768 - accuracy: 0.8831 - val_loss: 0.3361 - val_accuracy: 0.9088\n",
            "Epoch 62/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3860 - accuracy: 0.8797 - val_loss: 0.3282 - val_accuracy: 0.9099\n",
            "Epoch 63/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3583 - accuracy: 0.8864 - val_loss: 0.3239 - val_accuracy: 0.9132\n",
            "Epoch 64/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3524 - accuracy: 0.8987 - val_loss: 0.3192 - val_accuracy: 0.9255\n",
            "Epoch 65/400\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.3566 - accuracy: 0.8886 - val_loss: 0.3162 - val_accuracy: 0.9166\n",
            "Epoch 66/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3286 - accuracy: 0.8942 - val_loss: 0.3111 - val_accuracy: 0.9143\n",
            "Epoch 67/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3378 - accuracy: 0.9020 - val_loss: 0.3102 - val_accuracy: 0.9177\n",
            "Epoch 68/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3595 - accuracy: 0.8886 - val_loss: 0.3144 - val_accuracy: 0.9099\n",
            "Epoch 69/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3330 - accuracy: 0.9076 - val_loss: 0.3088 - val_accuracy: 0.9088\n",
            "Epoch 70/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3512 - accuracy: 0.8964 - val_loss: 0.3137 - val_accuracy: 0.9221\n",
            "Epoch 71/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3414 - accuracy: 0.8898 - val_loss: 0.3229 - val_accuracy: 0.9132\n",
            "Epoch 72/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3289 - accuracy: 0.8998 - val_loss: 0.2975 - val_accuracy: 0.9266\n",
            "Epoch 73/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2973 - accuracy: 0.9143 - val_loss: 0.2926 - val_accuracy: 0.9288\n",
            "Epoch 74/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2997 - accuracy: 0.9131 - val_loss: 0.2898 - val_accuracy: 0.9310\n",
            "Epoch 75/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3078 - accuracy: 0.9131 - val_loss: 0.2865 - val_accuracy: 0.9321\n",
            "Epoch 76/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2996 - accuracy: 0.9154 - val_loss: 0.2842 - val_accuracy: 0.9255\n",
            "Epoch 77/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3048 - accuracy: 0.9098 - val_loss: 0.2859 - val_accuracy: 0.9210\n",
            "Epoch 78/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2898 - accuracy: 0.9165 - val_loss: 0.2886 - val_accuracy: 0.9221\n",
            "Epoch 79/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2901 - accuracy: 0.9020 - val_loss: 0.2775 - val_accuracy: 0.9299\n",
            "Epoch 80/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2770 - accuracy: 0.9254 - val_loss: 0.2719 - val_accuracy: 0.9366\n",
            "Epoch 81/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3037 - accuracy: 0.9098 - val_loss: 0.2766 - val_accuracy: 0.9266\n",
            "Epoch 82/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2913 - accuracy: 0.9065 - val_loss: 0.2693 - val_accuracy: 0.9310\n",
            "Epoch 83/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2681 - accuracy: 0.9209 - val_loss: 0.2675 - val_accuracy: 0.9299\n",
            "Epoch 84/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2779 - accuracy: 0.9131 - val_loss: 0.2664 - val_accuracy: 0.9299\n",
            "Epoch 85/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2718 - accuracy: 0.9254 - val_loss: 0.2609 - val_accuracy: 0.9333\n",
            "Epoch 86/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2656 - accuracy: 0.9187 - val_loss: 0.2605 - val_accuracy: 0.9321\n",
            "Epoch 87/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2751 - accuracy: 0.9120 - val_loss: 0.2575 - val_accuracy: 0.9344\n",
            "Epoch 88/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2687 - accuracy: 0.9198 - val_loss: 0.2594 - val_accuracy: 0.9344\n",
            "Epoch 89/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2816 - accuracy: 0.9198 - val_loss: 0.2541 - val_accuracy: 0.9333\n",
            "Epoch 90/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2639 - accuracy: 0.9243 - val_loss: 0.2564 - val_accuracy: 0.9310\n",
            "Epoch 91/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2629 - accuracy: 0.9209 - val_loss: 0.2487 - val_accuracy: 0.9422\n",
            "Epoch 92/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2516 - accuracy: 0.9220 - val_loss: 0.2515 - val_accuracy: 0.9355\n",
            "Epoch 93/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2546 - accuracy: 0.9232 - val_loss: 0.2481 - val_accuracy: 0.9355\n",
            "Epoch 94/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2632 - accuracy: 0.9198 - val_loss: 0.2408 - val_accuracy: 0.9410\n",
            "Epoch 95/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2495 - accuracy: 0.9243 - val_loss: 0.2404 - val_accuracy: 0.9433\n",
            "Epoch 96/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2457 - accuracy: 0.9287 - val_loss: 0.2409 - val_accuracy: 0.9399\n",
            "Epoch 97/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2370 - accuracy: 0.9332 - val_loss: 0.2402 - val_accuracy: 0.9388\n",
            "Epoch 98/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2209 - accuracy: 0.9343 - val_loss: 0.2345 - val_accuracy: 0.9410\n",
            "Epoch 99/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2368 - accuracy: 0.9243 - val_loss: 0.2339 - val_accuracy: 0.9399\n",
            "Epoch 100/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2149 - accuracy: 0.9365 - val_loss: 0.2334 - val_accuracy: 0.9399\n",
            "Epoch 101/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2332 - accuracy: 0.9354 - val_loss: 0.2323 - val_accuracy: 0.9477\n",
            "Epoch 102/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2291 - accuracy: 0.9243 - val_loss: 0.2316 - val_accuracy: 0.9433\n",
            "Epoch 103/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2275 - accuracy: 0.9388 - val_loss: 0.2332 - val_accuracy: 0.9377\n",
            "Epoch 104/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2319 - accuracy: 0.9254 - val_loss: 0.2354 - val_accuracy: 0.9388\n",
            "Epoch 105/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1989 - accuracy: 0.9443 - val_loss: 0.2338 - val_accuracy: 0.9433\n",
            "Epoch 106/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2247 - accuracy: 0.9298 - val_loss: 0.2298 - val_accuracy: 0.9444\n",
            "Epoch 107/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2162 - accuracy: 0.9265 - val_loss: 0.2279 - val_accuracy: 0.9444\n",
            "Epoch 108/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1917 - accuracy: 0.9432 - val_loss: 0.2294 - val_accuracy: 0.9433\n",
            "Epoch 109/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2163 - accuracy: 0.9432 - val_loss: 0.2305 - val_accuracy: 0.9477\n",
            "Epoch 110/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2309 - accuracy: 0.9298 - val_loss: 0.2487 - val_accuracy: 0.9344\n",
            "Epoch 111/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2059 - accuracy: 0.9321 - val_loss: 0.2249 - val_accuracy: 0.9422\n",
            "Epoch 112/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1843 - accuracy: 0.9521 - val_loss: 0.2220 - val_accuracy: 0.9499\n",
            "Epoch 113/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2081 - accuracy: 0.9388 - val_loss: 0.2207 - val_accuracy: 0.9499\n",
            "Epoch 114/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1856 - accuracy: 0.9432 - val_loss: 0.2170 - val_accuracy: 0.9455\n",
            "Epoch 115/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2067 - accuracy: 0.9310 - val_loss: 0.2162 - val_accuracy: 0.9477\n",
            "Epoch 116/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1987 - accuracy: 0.9276 - val_loss: 0.2240 - val_accuracy: 0.9499\n",
            "Epoch 117/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1896 - accuracy: 0.9488 - val_loss: 0.2218 - val_accuracy: 0.9444\n",
            "Epoch 118/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1973 - accuracy: 0.9410 - val_loss: 0.2181 - val_accuracy: 0.9399\n",
            "Epoch 119/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2001 - accuracy: 0.9354 - val_loss: 0.2147 - val_accuracy: 0.9511\n",
            "Epoch 120/400\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.1839 - accuracy: 0.9432 - val_loss: 0.2142 - val_accuracy: 0.9422\n",
            "Epoch 121/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1793 - accuracy: 0.9477 - val_loss: 0.2130 - val_accuracy: 0.9433\n",
            "Epoch 122/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1813 - accuracy: 0.9499 - val_loss: 0.2095 - val_accuracy: 0.9433\n",
            "Epoch 123/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1860 - accuracy: 0.9432 - val_loss: 0.2099 - val_accuracy: 0.9511\n",
            "Epoch 124/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1758 - accuracy: 0.9376 - val_loss: 0.2089 - val_accuracy: 0.9499\n",
            "Epoch 125/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1957 - accuracy: 0.9432 - val_loss: 0.2124 - val_accuracy: 0.9488\n",
            "Epoch 126/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1625 - accuracy: 0.9621 - val_loss: 0.2095 - val_accuracy: 0.9522\n",
            "Epoch 127/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1782 - accuracy: 0.9454 - val_loss: 0.2154 - val_accuracy: 0.9444\n",
            "Epoch 128/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1903 - accuracy: 0.9410 - val_loss: 0.2164 - val_accuracy: 0.9444\n",
            "Epoch 129/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1898 - accuracy: 0.9521 - val_loss: 0.2128 - val_accuracy: 0.9499\n",
            "Epoch 130/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1891 - accuracy: 0.9410 - val_loss: 0.2045 - val_accuracy: 0.9477\n",
            "Epoch 131/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1719 - accuracy: 0.9488 - val_loss: 0.2049 - val_accuracy: 0.9488\n",
            "Epoch 132/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1741 - accuracy: 0.9465 - val_loss: 0.2047 - val_accuracy: 0.9488\n",
            "Epoch 133/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1633 - accuracy: 0.9543 - val_loss: 0.2043 - val_accuracy: 0.9499\n",
            "Epoch 134/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1737 - accuracy: 0.9488 - val_loss: 0.2019 - val_accuracy: 0.9477\n",
            "Epoch 135/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.9477 - val_loss: 0.2051 - val_accuracy: 0.9522\n",
            "Epoch 136/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1470 - accuracy: 0.9566 - val_loss: 0.2034 - val_accuracy: 0.9433\n",
            "Epoch 137/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9555 - val_loss: 0.2047 - val_accuracy: 0.9444\n",
            "Epoch 138/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1598 - accuracy: 0.9588 - val_loss: 0.2034 - val_accuracy: 0.9444\n",
            "Epoch 139/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1615 - accuracy: 0.9421 - val_loss: 0.2053 - val_accuracy: 0.9477\n",
            "Epoch 140/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.9454 - val_loss: 0.2145 - val_accuracy: 0.9455\n",
            "Epoch 141/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1434 - accuracy: 0.9655 - val_loss: 0.2021 - val_accuracy: 0.9477\n",
            "Epoch 142/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1536 - accuracy: 0.9543 - val_loss: 0.2023 - val_accuracy: 0.9511\n",
            "Epoch 143/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1537 - accuracy: 0.9532 - val_loss: 0.1986 - val_accuracy: 0.9488\n",
            "Epoch 144/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1456 - accuracy: 0.9566 - val_loss: 0.2009 - val_accuracy: 0.9466\n",
            "Epoch 145/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1516 - accuracy: 0.9566 - val_loss: 0.2000 - val_accuracy: 0.9455\n",
            "Epoch 146/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1436 - accuracy: 0.9610 - val_loss: 0.1967 - val_accuracy: 0.9488\n",
            "Epoch 147/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9566 - val_loss: 0.1955 - val_accuracy: 0.9477\n",
            "Epoch 148/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9566 - val_loss: 0.1959 - val_accuracy: 0.9488\n",
            "Epoch 149/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1358 - accuracy: 0.9621 - val_loss: 0.1988 - val_accuracy: 0.9511\n",
            "Epoch 150/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1286 - accuracy: 0.9666 - val_loss: 0.1956 - val_accuracy: 0.9511\n",
            "Epoch 151/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1459 - accuracy: 0.9599 - val_loss: 0.1942 - val_accuracy: 0.9488\n",
            "Epoch 152/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9666 - val_loss: 0.1961 - val_accuracy: 0.9488\n",
            "Epoch 153/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1433 - accuracy: 0.9499 - val_loss: 0.1933 - val_accuracy: 0.9511\n",
            "Epoch 154/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1279 - accuracy: 0.9588 - val_loss: 0.1981 - val_accuracy: 0.9499\n",
            "Epoch 155/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1436 - accuracy: 0.9521 - val_loss: 0.2129 - val_accuracy: 0.9455\n",
            "Epoch 156/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1579 - accuracy: 0.9532 - val_loss: 0.1982 - val_accuracy: 0.9522\n",
            "Epoch 157/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1272 - accuracy: 0.9543 - val_loss: 0.1907 - val_accuracy: 0.9499\n",
            "Epoch 158/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1475 - accuracy: 0.9588 - val_loss: 0.1921 - val_accuracy: 0.9488\n",
            "Epoch 159/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1297 - accuracy: 0.9655 - val_loss: 0.1936 - val_accuracy: 0.9533\n",
            "Epoch 160/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1319 - accuracy: 0.9588 - val_loss: 0.1950 - val_accuracy: 0.9544\n",
            "Epoch 161/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1340 - accuracy: 0.9621 - val_loss: 0.1956 - val_accuracy: 0.9533\n",
            "Epoch 162/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1235 - accuracy: 0.9688 - val_loss: 0.1954 - val_accuracy: 0.9522\n",
            "Epoch 163/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1276 - accuracy: 0.9666 - val_loss: 0.1948 - val_accuracy: 0.9544\n",
            "Epoch 164/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1474 - accuracy: 0.9532 - val_loss: 0.1933 - val_accuracy: 0.9533\n",
            "Epoch 165/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1442 - accuracy: 0.9577 - val_loss: 0.1928 - val_accuracy: 0.9533\n",
            "Epoch 166/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1381 - accuracy: 0.9577 - val_loss: 0.2094 - val_accuracy: 0.9410\n",
            "Epoch 167/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1185 - accuracy: 0.9644 - val_loss: 0.1976 - val_accuracy: 0.9544\n",
            "Epoch 168/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1220 - accuracy: 0.9621 - val_loss: 0.1929 - val_accuracy: 0.9555\n",
            "Epoch 169/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1235 - accuracy: 0.9577 - val_loss: 0.2007 - val_accuracy: 0.9488\n",
            "Epoch 170/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1269 - accuracy: 0.9599 - val_loss: 0.1950 - val_accuracy: 0.9555\n",
            "Epoch 171/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1277 - accuracy: 0.9621 - val_loss: 0.1917 - val_accuracy: 0.9544\n",
            "Epoch 172/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1252 - accuracy: 0.9688 - val_loss: 0.1920 - val_accuracy: 0.9544\n",
            "Epoch 173/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1140 - accuracy: 0.9699 - val_loss: 0.1929 - val_accuracy: 0.9544\n",
            "Epoch 174/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1155 - accuracy: 0.9722 - val_loss: 0.1931 - val_accuracy: 0.9511\n",
            "Epoch 175/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1026 - accuracy: 0.9722 - val_loss: 0.1906 - val_accuracy: 0.9533\n",
            "Epoch 176/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1124 - accuracy: 0.9688 - val_loss: 0.1874 - val_accuracy: 0.9544\n",
            "Epoch 177/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1293 - accuracy: 0.9588 - val_loss: 0.1896 - val_accuracy: 0.9522\n",
            "Epoch 178/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1202 - accuracy: 0.9710 - val_loss: 0.1914 - val_accuracy: 0.9544\n",
            "Epoch 179/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1036 - accuracy: 0.9744 - val_loss: 0.1900 - val_accuracy: 0.9544\n",
            "Epoch 180/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1147 - accuracy: 0.9644 - val_loss: 0.1901 - val_accuracy: 0.9566\n",
            "Epoch 181/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1127 - accuracy: 0.9722 - val_loss: 0.1894 - val_accuracy: 0.9555\n",
            "Epoch 182/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1134 - accuracy: 0.9677 - val_loss: 0.1870 - val_accuracy: 0.9533\n",
            "Epoch 183/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1169 - accuracy: 0.9644 - val_loss: 0.1872 - val_accuracy: 0.9544\n",
            "Epoch 184/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1016 - accuracy: 0.9722 - val_loss: 0.1889 - val_accuracy: 0.9544\n",
            "Epoch 185/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1145 - accuracy: 0.9655 - val_loss: 0.1929 - val_accuracy: 0.9522\n",
            "Epoch 186/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1086 - accuracy: 0.9688 - val_loss: 0.1924 - val_accuracy: 0.9533\n",
            "Epoch 187/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1087 - accuracy: 0.9610 - val_loss: 0.1870 - val_accuracy: 0.9566\n",
            "Epoch 188/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 0.9621 - val_loss: 0.1892 - val_accuracy: 0.9577\n",
            "Epoch 189/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1020 - accuracy: 0.9699 - val_loss: 0.1898 - val_accuracy: 0.9544\n",
            "Epoch 190/400\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1123 - accuracy: 0.9688 - val_loss: 0.1892 - val_accuracy: 0.9555\n",
            "Epoch 191/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1083 - accuracy: 0.9644 - val_loss: 0.1873 - val_accuracy: 0.9544\n",
            "Epoch 192/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1012 - accuracy: 0.9733 - val_loss: 0.1918 - val_accuracy: 0.9522\n",
            "Epoch 193/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1099 - accuracy: 0.9666 - val_loss: 0.1882 - val_accuracy: 0.9566\n",
            "Epoch 194/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9677 - val_loss: 0.1847 - val_accuracy: 0.9566\n",
            "Epoch 195/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1014 - accuracy: 0.9699 - val_loss: 0.1844 - val_accuracy: 0.9577\n",
            "Epoch 196/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9599 - val_loss: 0.1854 - val_accuracy: 0.9544\n",
            "Epoch 197/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1161 - accuracy: 0.9621 - val_loss: 0.1833 - val_accuracy: 0.9544\n",
            "Epoch 198/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0979 - accuracy: 0.9677 - val_loss: 0.1858 - val_accuracy: 0.9511\n",
            "Epoch 199/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0961 - accuracy: 0.9755 - val_loss: 0.1851 - val_accuracy: 0.9555\n",
            "Epoch 200/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0981 - accuracy: 0.9699 - val_loss: 0.1827 - val_accuracy: 0.9555\n",
            "Epoch 201/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0989 - accuracy: 0.9710 - val_loss: 0.1825 - val_accuracy: 0.9544\n",
            "Epoch 202/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0951 - accuracy: 0.9710 - val_loss: 0.1825 - val_accuracy: 0.9555\n",
            "Epoch 203/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1039 - accuracy: 0.9644 - val_loss: 0.1844 - val_accuracy: 0.9533\n",
            "Epoch 204/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0985 - accuracy: 0.9733 - val_loss: 0.1830 - val_accuracy: 0.9566\n",
            "Epoch 205/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0891 - accuracy: 0.9699 - val_loss: 0.1842 - val_accuracy: 0.9555\n",
            "Epoch 206/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0949 - accuracy: 0.9777 - val_loss: 0.1860 - val_accuracy: 0.9544\n",
            "Epoch 207/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1105 - accuracy: 0.9644 - val_loss: 0.1844 - val_accuracy: 0.9555\n",
            "Epoch 208/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0933 - accuracy: 0.9710 - val_loss: 0.1836 - val_accuracy: 0.9544\n",
            "Epoch 209/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0952 - accuracy: 0.9710 - val_loss: 0.1824 - val_accuracy: 0.9566\n",
            "Epoch 210/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0900 - accuracy: 0.9755 - val_loss: 0.1831 - val_accuracy: 0.9555\n",
            "Epoch 211/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0866 - accuracy: 0.9777 - val_loss: 0.1840 - val_accuracy: 0.9588\n",
            "Epoch 212/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0845 - accuracy: 0.9755 - val_loss: 0.1869 - val_accuracy: 0.9566\n",
            "Epoch 213/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0831 - accuracy: 0.9777 - val_loss: 0.1873 - val_accuracy: 0.9544\n",
            "Epoch 214/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0954 - accuracy: 0.9655 - val_loss: 0.1877 - val_accuracy: 0.9544\n",
            "Epoch 215/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0812 - accuracy: 0.9811 - val_loss: 0.1866 - val_accuracy: 0.9522\n",
            "Epoch 216/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0875 - accuracy: 0.9733 - val_loss: 0.1882 - val_accuracy: 0.9533\n",
            "Epoch 217/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0922 - accuracy: 0.9744 - val_loss: 0.1804 - val_accuracy: 0.9566\n",
            "Epoch 218/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0875 - accuracy: 0.9755 - val_loss: 0.1830 - val_accuracy: 0.9577\n",
            "Epoch 219/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1014 - accuracy: 0.9655 - val_loss: 0.1832 - val_accuracy: 0.9566\n",
            "Epoch 220/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0732 - accuracy: 0.9822 - val_loss: 0.1852 - val_accuracy: 0.9544\n",
            "Epoch 221/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1072 - accuracy: 0.9588 - val_loss: 0.1923 - val_accuracy: 0.9544\n",
            "Epoch 222/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0867 - accuracy: 0.9800 - val_loss: 0.1852 - val_accuracy: 0.9555\n",
            "Epoch 223/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0929 - accuracy: 0.9710 - val_loss: 0.1893 - val_accuracy: 0.9544\n",
            "Epoch 224/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0995 - accuracy: 0.9688 - val_loss: 0.1841 - val_accuracy: 0.9566\n",
            "Epoch 225/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9733 - val_loss: 0.1890 - val_accuracy: 0.9566\n",
            "Epoch 226/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1165 - accuracy: 0.9610 - val_loss: 0.2083 - val_accuracy: 0.9455\n",
            "Epoch 227/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1035 - accuracy: 0.9688 - val_loss: 0.2024 - val_accuracy: 0.9544\n",
            "Epoch 228/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1046 - accuracy: 0.9733 - val_loss: 0.1911 - val_accuracy: 0.9566\n",
            "Epoch 229/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0948 - accuracy: 0.9710 - val_loss: 0.1892 - val_accuracy: 0.9555\n",
            "Epoch 230/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9777 - val_loss: 0.1874 - val_accuracy: 0.9555\n",
            "Epoch 231/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0788 - accuracy: 0.9811 - val_loss: 0.1879 - val_accuracy: 0.9555\n",
            "Epoch 232/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0828 - accuracy: 0.9811 - val_loss: 0.1856 - val_accuracy: 0.9577\n",
            "Epoch 233/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0765 - accuracy: 0.9833 - val_loss: 0.1840 - val_accuracy: 0.9555\n",
            "Epoch 234/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0850 - accuracy: 0.9744 - val_loss: 0.1846 - val_accuracy: 0.9555\n",
            "Epoch 235/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0719 - accuracy: 0.9855 - val_loss: 0.1853 - val_accuracy: 0.9544\n",
            "Epoch 236/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0818 - accuracy: 0.9811 - val_loss: 0.1822 - val_accuracy: 0.9577\n",
            "Epoch 237/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0759 - accuracy: 0.9755 - val_loss: 0.1842 - val_accuracy: 0.9577\n",
            "Epoch 238/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0868 - accuracy: 0.9710 - val_loss: 0.1937 - val_accuracy: 0.9544\n",
            "Epoch 239/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0919 - accuracy: 0.9722 - val_loss: 0.1889 - val_accuracy: 0.9588\n",
            "Epoch 240/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0819 - accuracy: 0.9800 - val_loss: 0.1795 - val_accuracy: 0.9577\n",
            "Epoch 241/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9811 - val_loss: 0.1790 - val_accuracy: 0.9577\n",
            "Epoch 242/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0637 - accuracy: 0.9866 - val_loss: 0.1859 - val_accuracy: 0.9588\n",
            "Epoch 243/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0728 - accuracy: 0.9766 - val_loss: 0.1835 - val_accuracy: 0.9600\n",
            "Epoch 244/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 0.9800 - val_loss: 0.1857 - val_accuracy: 0.9566\n",
            "Epoch 245/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0954 - accuracy: 0.9655 - val_loss: 0.1849 - val_accuracy: 0.9577\n",
            "Epoch 246/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0834 - accuracy: 0.9744 - val_loss: 0.1802 - val_accuracy: 0.9588\n",
            "Epoch 247/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0703 - accuracy: 0.9800 - val_loss: 0.1814 - val_accuracy: 0.9577\n",
            "Epoch 248/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0787 - accuracy: 0.9755 - val_loss: 0.1822 - val_accuracy: 0.9588\n",
            "Epoch 249/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0647 - accuracy: 0.9866 - val_loss: 0.1916 - val_accuracy: 0.9511\n",
            "Epoch 250/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 0.9822 - val_loss: 0.1853 - val_accuracy: 0.9566\n",
            "Epoch 251/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0728 - accuracy: 0.9777 - val_loss: 0.1832 - val_accuracy: 0.9566\n",
            "Epoch 252/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0585 - accuracy: 0.9878 - val_loss: 0.1830 - val_accuracy: 0.9566\n",
            "Epoch 253/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0722 - accuracy: 0.9744 - val_loss: 0.1825 - val_accuracy: 0.9566\n",
            "Epoch 254/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0785 - accuracy: 0.9788 - val_loss: 0.1878 - val_accuracy: 0.9577\n",
            "Epoch 255/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0786 - accuracy: 0.9777 - val_loss: 0.1819 - val_accuracy: 0.9588\n",
            "Epoch 256/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0775 - accuracy: 0.9766 - val_loss: 0.1875 - val_accuracy: 0.9588\n",
            "Epoch 257/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0785 - accuracy: 0.9744 - val_loss: 0.1845 - val_accuracy: 0.9566\n",
            "Epoch 258/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0724 - accuracy: 0.9800 - val_loss: 0.1824 - val_accuracy: 0.9577\n",
            "Epoch 259/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0677 - accuracy: 0.9788 - val_loss: 0.1801 - val_accuracy: 0.9588\n",
            "Epoch 260/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9822 - val_loss: 0.1816 - val_accuracy: 0.9588\n",
            "Epoch 261/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0737 - accuracy: 0.9777 - val_loss: 0.1795 - val_accuracy: 0.9577\n",
            "Epoch 262/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0692 - accuracy: 0.9822 - val_loss: 0.1810 - val_accuracy: 0.9588\n",
            "Epoch 263/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0735 - accuracy: 0.9766 - val_loss: 0.1821 - val_accuracy: 0.9577\n",
            "Epoch 264/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0746 - accuracy: 0.9755 - val_loss: 0.1802 - val_accuracy: 0.9566\n",
            "Epoch 265/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0666 - accuracy: 0.9822 - val_loss: 0.1788 - val_accuracy: 0.9555\n",
            "Epoch 266/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0664 - accuracy: 0.9800 - val_loss: 0.1802 - val_accuracy: 0.9577\n",
            "Epoch 267/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0719 - accuracy: 0.9822 - val_loss: 0.1866 - val_accuracy: 0.9511\n",
            "Epoch 268/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0662 - accuracy: 0.9800 - val_loss: 0.1831 - val_accuracy: 0.9544\n",
            "Epoch 269/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0651 - accuracy: 0.9811 - val_loss: 0.1800 - val_accuracy: 0.9577\n",
            "Epoch 270/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0636 - accuracy: 0.9777 - val_loss: 0.1776 - val_accuracy: 0.9600\n",
            "Epoch 271/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0864 - accuracy: 0.9744 - val_loss: 0.1808 - val_accuracy: 0.9566\n",
            "Epoch 272/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0823 - accuracy: 0.9722 - val_loss: 0.1815 - val_accuracy: 0.9588\n",
            "Epoch 273/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0732 - accuracy: 0.9833 - val_loss: 0.1783 - val_accuracy: 0.9566\n",
            "Epoch 274/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0711 - accuracy: 0.9766 - val_loss: 0.1794 - val_accuracy: 0.9566\n",
            "Epoch 275/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9777 - val_loss: 0.1797 - val_accuracy: 0.9566\n",
            "Epoch 276/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0628 - accuracy: 0.9788 - val_loss: 0.1784 - val_accuracy: 0.9566\n",
            "Epoch 277/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 0.9822 - val_loss: 0.1740 - val_accuracy: 0.9588\n",
            "Epoch 278/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0545 - accuracy: 0.9900 - val_loss: 0.1770 - val_accuracy: 0.9588\n",
            "Epoch 279/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0682 - accuracy: 0.9766 - val_loss: 0.1757 - val_accuracy: 0.9588\n",
            "Epoch 280/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0648 - accuracy: 0.9800 - val_loss: 0.1801 - val_accuracy: 0.9600\n",
            "Epoch 281/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0612 - accuracy: 0.9800 - val_loss: 0.1810 - val_accuracy: 0.9588\n",
            "Epoch 282/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0683 - accuracy: 0.9788 - val_loss: 0.1791 - val_accuracy: 0.9600\n",
            "Epoch 283/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0667 - accuracy: 0.9788 - val_loss: 0.1776 - val_accuracy: 0.9588\n",
            "Epoch 284/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0592 - accuracy: 0.9855 - val_loss: 0.1778 - val_accuracy: 0.9600\n",
            "Epoch 285/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9889 - val_loss: 0.1795 - val_accuracy: 0.9588\n",
            "Epoch 286/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0683 - accuracy: 0.9833 - val_loss: 0.1829 - val_accuracy: 0.9577\n",
            "Epoch 287/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0685 - accuracy: 0.9811 - val_loss: 0.1822 - val_accuracy: 0.9588\n",
            "Epoch 288/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0613 - accuracy: 0.9844 - val_loss: 0.1851 - val_accuracy: 0.9566\n",
            "Epoch 289/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0598 - accuracy: 0.9788 - val_loss: 0.1766 - val_accuracy: 0.9577\n",
            "Epoch 290/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0548 - accuracy: 0.9855 - val_loss: 0.1772 - val_accuracy: 0.9588\n",
            "Epoch 291/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.1846 - val_accuracy: 0.9566\n",
            "Epoch 292/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 0.1828 - val_accuracy: 0.9588\n",
            "Epoch 293/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0531 - accuracy: 0.9889 - val_loss: 0.1852 - val_accuracy: 0.9555\n",
            "Epoch 294/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0630 - accuracy: 0.9844 - val_loss: 0.1805 - val_accuracy: 0.9588\n",
            "Epoch 295/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0592 - accuracy: 0.9889 - val_loss: 0.1804 - val_accuracy: 0.9588\n",
            "Epoch 296/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.1810 - val_accuracy: 0.9588\n",
            "Epoch 297/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0588 - accuracy: 0.9811 - val_loss: 0.1807 - val_accuracy: 0.9600\n",
            "Epoch 298/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0542 - accuracy: 0.9855 - val_loss: 0.1798 - val_accuracy: 0.9600\n",
            "Epoch 299/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0503 - accuracy: 0.9866 - val_loss: 0.1803 - val_accuracy: 0.9588\n",
            "Epoch 300/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0576 - accuracy: 0.9844 - val_loss: 0.1805 - val_accuracy: 0.9577\n",
            "Epoch 301/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9866 - val_loss: 0.1781 - val_accuracy: 0.9588\n",
            "Epoch 302/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.9878 - val_loss: 0.1836 - val_accuracy: 0.9566\n",
            "Epoch 303/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.9788 - val_loss: 0.2035 - val_accuracy: 0.9522\n",
            "Epoch 304/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0693 - accuracy: 0.9766 - val_loss: 0.1899 - val_accuracy: 0.9577\n",
            "Epoch 305/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9822 - val_loss: 0.1885 - val_accuracy: 0.9611\n",
            "Epoch 306/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0507 - accuracy: 0.9811 - val_loss: 0.1850 - val_accuracy: 0.9577\n",
            "Epoch 307/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0553 - accuracy: 0.9855 - val_loss: 0.1897 - val_accuracy: 0.9566\n",
            "Epoch 308/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0417 - accuracy: 0.9955 - val_loss: 0.1822 - val_accuracy: 0.9577\n",
            "Epoch 309/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0573 - accuracy: 0.9844 - val_loss: 0.1827 - val_accuracy: 0.9577\n",
            "Epoch 310/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9878 - val_loss: 0.1897 - val_accuracy: 0.9577\n",
            "Epoch 311/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0547 - accuracy: 0.9844 - val_loss: 0.1923 - val_accuracy: 0.9566\n",
            "Epoch 312/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0671 - accuracy: 0.9777 - val_loss: 0.1852 - val_accuracy: 0.9600\n",
            "Epoch 313/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9855 - val_loss: 0.1834 - val_accuracy: 0.9588\n",
            "Epoch 314/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0504 - accuracy: 0.9844 - val_loss: 0.1838 - val_accuracy: 0.9588\n",
            "Epoch 315/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9855 - val_loss: 0.1832 - val_accuracy: 0.9600\n",
            "Epoch 316/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0587 - accuracy: 0.9844 - val_loss: 0.1858 - val_accuracy: 0.9588\n",
            "Epoch 317/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0669 - accuracy: 0.9755 - val_loss: 0.2393 - val_accuracy: 0.9410\n",
            "Epoch 318/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9811 - val_loss: 0.1920 - val_accuracy: 0.9566\n",
            "Epoch 319/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.1875 - val_accuracy: 0.9600\n",
            "Epoch 320/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.1866 - val_accuracy: 0.9588\n",
            "Epoch 321/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9911 - val_loss: 0.1870 - val_accuracy: 0.9588\n",
            "Epoch 322/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9933 - val_loss: 0.1853 - val_accuracy: 0.9600\n",
            "Epoch 323/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0620 - accuracy: 0.9800 - val_loss: 0.1850 - val_accuracy: 0.9600\n",
            "Epoch 324/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0482 - accuracy: 0.9822 - val_loss: 0.1905 - val_accuracy: 0.9577\n",
            "Epoch 325/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0575 - accuracy: 0.9855 - val_loss: 0.1834 - val_accuracy: 0.9600\n",
            "Epoch 326/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9878 - val_loss: 0.1848 - val_accuracy: 0.9622\n",
            "Epoch 327/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.1853 - val_accuracy: 0.9577\n",
            "Epoch 328/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0598 - accuracy: 0.9777 - val_loss: 0.1855 - val_accuracy: 0.9588\n",
            "Epoch 329/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0686 - accuracy: 0.9800 - val_loss: 0.1889 - val_accuracy: 0.9588\n",
            "Epoch 330/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0476 - accuracy: 0.9900 - val_loss: 0.1866 - val_accuracy: 0.9588\n",
            "Epoch 331/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9866 - val_loss: 0.1883 - val_accuracy: 0.9577\n",
            "Epoch 332/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.1873 - val_accuracy: 0.9588\n",
            "Epoch 333/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0533 - accuracy: 0.9866 - val_loss: 0.1862 - val_accuracy: 0.9600\n",
            "Epoch 334/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0396 - accuracy: 0.9911 - val_loss: 0.1904 - val_accuracy: 0.9544\n",
            "Epoch 335/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.1870 - val_accuracy: 0.9588\n",
            "Epoch 336/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 0.9900 - val_loss: 0.1850 - val_accuracy: 0.9611\n",
            "Epoch 337/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9944 - val_loss: 0.1833 - val_accuracy: 0.9600\n",
            "Epoch 338/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0424 - accuracy: 0.9878 - val_loss: 0.1859 - val_accuracy: 0.9588\n",
            "Epoch 339/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0438 - accuracy: 0.9900 - val_loss: 0.1875 - val_accuracy: 0.9588\n",
            "Epoch 340/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0529 - accuracy: 0.9878 - val_loss: 0.1823 - val_accuracy: 0.9600\n",
            "Epoch 341/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0522 - accuracy: 0.9866 - val_loss: 0.1826 - val_accuracy: 0.9588\n",
            "Epoch 342/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0390 - accuracy: 0.9933 - val_loss: 0.1844 - val_accuracy: 0.9611\n",
            "Epoch 343/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.9900 - val_loss: 0.1855 - val_accuracy: 0.9600\n",
            "Epoch 344/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0622 - accuracy: 0.9800 - val_loss: 0.1837 - val_accuracy: 0.9588\n",
            "Epoch 345/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 0.1845 - val_accuracy: 0.9577\n",
            "Epoch 346/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0427 - accuracy: 0.9878 - val_loss: 0.1839 - val_accuracy: 0.9588\n",
            "Epoch 347/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9933 - val_loss: 0.1842 - val_accuracy: 0.9600\n",
            "Epoch 348/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0411 - accuracy: 0.9889 - val_loss: 0.1859 - val_accuracy: 0.9588\n",
            "Epoch 349/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.1848 - val_accuracy: 0.9600\n",
            "Epoch 350/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0365 - accuracy: 0.9911 - val_loss: 0.1825 - val_accuracy: 0.9611\n",
            "Epoch 351/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 0.9911 - val_loss: 0.1844 - val_accuracy: 0.9600\n",
            "Epoch 352/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0316 - accuracy: 0.9944 - val_loss: 0.1875 - val_accuracy: 0.9577\n",
            "Epoch 353/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0421 - accuracy: 0.9878 - val_loss: 0.1870 - val_accuracy: 0.9600\n",
            "Epoch 354/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0398 - accuracy: 0.9889 - val_loss: 0.1860 - val_accuracy: 0.9611\n",
            "Epoch 355/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9889 - val_loss: 0.1874 - val_accuracy: 0.9588\n",
            "Epoch 356/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0384 - accuracy: 0.9889 - val_loss: 0.1865 - val_accuracy: 0.9600\n",
            "Epoch 357/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9866 - val_loss: 0.1825 - val_accuracy: 0.9600\n",
            "Epoch 358/400\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.0396 - accuracy: 0.9900 - val_loss: 0.1823 - val_accuracy: 0.9611\n",
            "Epoch 359/400\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.0450 - accuracy: 0.9866 - val_loss: 0.1839 - val_accuracy: 0.9600\n",
            "Epoch 360/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9855 - val_loss: 0.1816 - val_accuracy: 0.9611\n",
            "Epoch 361/400\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.1810 - val_accuracy: 0.9611\n",
            "Epoch 362/400\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.1823 - val_accuracy: 0.9611\n",
            "Epoch 363/400\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9900 - val_loss: 0.1824 - val_accuracy: 0.9600\n",
            "Epoch 364/400\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0423 - accuracy: 0.9889 - val_loss: 0.1835 - val_accuracy: 0.9600\n",
            "Epoch 365/400\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 0.0355 - accuracy: 0.9933 - val_loss: 0.1819 - val_accuracy: 0.9611\n",
            "Epoch 366/400\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0345 - accuracy: 0.9933 - val_loss: 0.1817 - val_accuracy: 0.9600\n",
            "Epoch 367/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0441 - accuracy: 0.9866 - val_loss: 0.1831 - val_accuracy: 0.9600\n",
            "Epoch 368/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9900 - val_loss: 0.1838 - val_accuracy: 0.9600\n",
            "Epoch 369/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0383 - accuracy: 0.9911 - val_loss: 0.1837 - val_accuracy: 0.9600\n",
            "Epoch 370/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 0.9900 - val_loss: 0.1841 - val_accuracy: 0.9611\n",
            "Epoch 371/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 0.1832 - val_accuracy: 0.9577\n",
            "Epoch 372/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0381 - accuracy: 0.9911 - val_loss: 0.1840 - val_accuracy: 0.9588\n",
            "Epoch 373/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0413 - accuracy: 0.9844 - val_loss: 0.1846 - val_accuracy: 0.9611\n",
            "Epoch 374/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9900 - val_loss: 0.1860 - val_accuracy: 0.9600\n",
            "Epoch 375/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0388 - accuracy: 0.9922 - val_loss: 0.1860 - val_accuracy: 0.9611\n",
            "Epoch 376/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0439 - accuracy: 0.9878 - val_loss: 0.1869 - val_accuracy: 0.9577\n",
            "Epoch 377/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0269 - accuracy: 0.9967 - val_loss: 0.1856 - val_accuracy: 0.9588\n",
            "Epoch 378/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0397 - accuracy: 0.9889 - val_loss: 0.1854 - val_accuracy: 0.9622\n",
            "Epoch 379/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0338 - accuracy: 0.9900 - val_loss: 0.1868 - val_accuracy: 0.9611\n",
            "Epoch 380/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 0.1884 - val_accuracy: 0.9600\n",
            "Epoch 381/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 0.9911 - val_loss: 0.1863 - val_accuracy: 0.9611\n",
            "Epoch 382/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0322 - accuracy: 0.9933 - val_loss: 0.1879 - val_accuracy: 0.9600\n",
            "Epoch 383/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0356 - accuracy: 0.9922 - val_loss: 0.1891 - val_accuracy: 0.9588\n",
            "Epoch 384/400\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0284 - accuracy: 0.9955 - val_loss: 0.1888 - val_accuracy: 0.9611\n",
            "Epoch 385/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0394 - accuracy: 0.9866 - val_loss: 0.1891 - val_accuracy: 0.9600\n",
            "Epoch 386/400\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0516 - accuracy: 0.9866 - val_loss: 0.2095 - val_accuracy: 0.9499\n",
            "Epoch 387/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.1902 - val_accuracy: 0.9588\n",
            "Epoch 388/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 0.9866 - val_loss: 0.1888 - val_accuracy: 0.9600\n",
            "Epoch 389/400\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0358 - accuracy: 0.9911 - val_loss: 0.1821 - val_accuracy: 0.9622\n",
            "Epoch 390/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0317 - accuracy: 0.9922 - val_loss: 0.1830 - val_accuracy: 0.9611\n",
            "Epoch 391/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9922 - val_loss: 0.1819 - val_accuracy: 0.9611\n",
            "Epoch 392/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9889 - val_loss: 0.1824 - val_accuracy: 0.9588\n",
            "Epoch 393/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.9944 - val_loss: 0.1858 - val_accuracy: 0.9588\n",
            "Epoch 394/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9922 - val_loss: 0.1867 - val_accuracy: 0.9600\n",
            "Epoch 395/400\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0294 - accuracy: 0.9944 - val_loss: 0.1882 - val_accuracy: 0.9600\n",
            "Epoch 396/400\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0392 - accuracy: 0.9900 - val_loss: 0.1912 - val_accuracy: 0.9588\n",
            "Epoch 397/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0267 - accuracy: 0.9955 - val_loss: 0.1857 - val_accuracy: 0.9611\n",
            "Epoch 398/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.1861 - val_accuracy: 0.9611\n",
            "Epoch 399/400\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0293 - accuracy: 0.9933 - val_loss: 0.1871 - val_accuracy: 0.9611\n",
            "Epoch 400/400\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.1913 - val_accuracy: 0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Evaluating the Model</h1>**\n",
        "\n",
        "<li>Below, we evaluate the model on the training and testing sets, as well as calculate the generalization gap.</li>\n",
        "\n",
        "> <li>The generalization gap is the difference between a model's performance on training data and its performance on unseen data drawn from the same distribution.</li>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2kPIHAXao__t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on training set\n",
        "train_loss, train_acc = model.evaluate(X_train, Y_train)\n",
        "print(\"train_loss:\", train_loss)\n",
        "print(\"train_acc:\", train_acc)"
      ],
      "metadata": {
        "id": "OYiKV-ohOzQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1846911-9104-429b-d7aa-8974b028ec91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "train_loss: 0.006940165068954229\n",
            "train_acc: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print(\"train_loss:\", test_loss)\n",
        "print(\"train_acc:\", test_acc)"
      ],
      "metadata": {
        "id": "tmJYoYjHPGou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbafc5a-0c9f-4954-a145-f624c0ad22cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9600\n",
            "train_loss: 0.19132837653160095\n",
            "train_acc: 0.9599555134773254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate generalization gap\n",
        "generalization_gap = train_loss - test_loss\n",
        "print(\"Generalization gap:\", generalization_gap)"
      ],
      "metadata": {
        "id": "MelPcUyyPMEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ac55de-538f-47db-88bf-66de3bc57618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generalization gap: -0.18438821146264672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Task 2.</h1>**\n"
      ],
      "metadata": {
        "id": "qbzQ_hiOqLup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Dumping the Weights</h1>**\n",
        "\n",
        "<li>Below, we dump the weights by layer with a <code>for</code> loop iterating through each layer in the model. The following section returns the weights of all the layers in the model.</li>"
      ],
      "metadata": {
        "id": "JLQokX2BqYMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dump the weights\n",
        "weights = model.get_weights()\n",
        "\n",
        "for layer in model.layers:\n",
        "  print(layer.get_config(), layer.get_weights())\n",
        "\n"
      ],
      "metadata": {
        "id": "noc_qGKaGNKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b07f61-977f-4469-9c85-9c12809f7d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'input_layer', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 64), 'units': 64, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.18569665, -0.13193066,  0.03065346, ...,  0.17088158,\n",
            "         0.02900732,  0.07129903],\n",
            "       [-0.12495903,  0.4021446 , -1.173175  , ..., -1.1209896 ,\n",
            "        -1.2544267 , -0.05443404],\n",
            "       [ 0.02680123, -0.05601811, -0.13567044, ..., -0.4735934 ,\n",
            "        -0.29746637,  0.20889318],\n",
            "       ...,\n",
            "       [ 0.27661455, -0.11284597,  0.0430582 , ..., -0.24261628,\n",
            "         0.04455539,  0.4607982 ],\n",
            "       [-0.28287137,  0.63326216, -0.3496443 , ..., -0.18219794,\n",
            "        -0.15444973,  0.2128075 ],\n",
            "       [-0.5993885 ,  0.8601746 ,  0.4333951 , ..., -1.085406  ,\n",
            "         0.45762154,  1.1965693 ]], dtype=float32), array([ 0.03862518,  0.02445105,  0.00214798,  0.02415922,  0.05181784,\n",
            "        0.05679493,  0.00622749, -0.0105153 ,  0.02707648,  0.02867642,\n",
            "        0.01468152, -0.02520968, -0.00481326,  0.01642161,  0.01207096,\n",
            "        0.04362757, -0.02136022,  0.03631023,  0.02577472,  0.015639  ,\n",
            "        0.00950702,  0.04788986,  0.05354606,  0.01890212,  0.03193757,\n",
            "        0.00246206,  0.03169054, -0.00977024,  0.07662588,  0.07985146,\n",
            "       -0.014138  ,  0.04822342, -0.01212838,  0.01544729,  0.03031229,\n",
            "        0.0209592 ,  0.00383001,  0.03135595,  0.03309345,  0.0011258 ,\n",
            "        0.06345885,  0.05771468,  0.00964002,  0.04236372,  0.02763998,\n",
            "       -0.01114968, -0.01798598,  0.00264125,  0.02155494,  0.        ,\n",
            "        0.08351316,  0.04612904,  0.        , -0.01000862, -0.00992936,\n",
            "        0.02463575,  0.00697693,  0.        , -0.02076427, -0.03424769,\n",
            "        0.06284536,  0.03525687, -0.0061763 , -0.01480649], dtype=float32)]\n",
            "{'name': 'activation', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'} []\n",
            "{'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None} []\n",
            "{'name': 'hidden_layer', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.71988285,  0.7987533 , -0.14588332, ...,  0.9930648 ,\n",
            "        -0.8418903 ,  0.05989664],\n",
            "       [-0.04388879, -0.3312265 ,  0.6391312 , ...,  0.23909578,\n",
            "         0.9826405 , -0.03298265],\n",
            "       [ 0.15639944, -0.12850736,  0.0054281 , ..., -0.3538429 ,\n",
            "         0.3737378 ,  0.07091696],\n",
            "       ...,\n",
            "       [ 0.03748988,  0.826015  ,  0.01627248, ..., -0.4175545 ,\n",
            "         0.12543313,  0.43962026],\n",
            "       [-0.03330702, -0.23320909,  0.483733  , ..., -0.3041923 ,\n",
            "         0.52911955, -0.07058693],\n",
            "       [-0.06971946,  0.00627857, -0.47415066, ...,  0.5152738 ,\n",
            "         0.07679212,  0.7221851 ]], dtype=float32), array([ 1.1969082e-01,  1.7833152e-01,  1.1429913e-01,  1.9168578e-02,\n",
            "        1.5512307e-01,  1.5915808e-01,  1.0637063e-01,  2.6362741e-02,\n",
            "        2.4015787e-01,  6.3602664e-02,  1.6088918e-01,  1.0011130e-01,\n",
            "        2.3378630e-01,  1.1011102e-01,  2.3675400e-01,  2.9380381e-04,\n",
            "        1.9356622e-01,  1.5734118e-01,  1.5660290e-01,  1.3638958e-01,\n",
            "        1.6729566e-01,  6.5885663e-02,  1.0788829e-01,  1.8165192e-01,\n",
            "        1.1171172e-01,  2.9361919e-01,  4.8757084e-02,  1.7030214e-01,\n",
            "        1.9318545e-01,  1.5309225e-02,  1.8863857e-01,  5.7050612e-02,\n",
            "        2.1283811e-02,  7.8279421e-02,  1.6392156e-02,  1.1547578e-01,\n",
            "        1.7739287e-01,  9.2036977e-02,  1.0438314e-01, -1.1354206e-01,\n",
            "        2.3904803e-01, -5.5353079e-02,  1.4098956e-02,  3.0741838e-01,\n",
            "        7.1031459e-02,  1.0705975e-02,  2.8365653e-02, -1.3195146e-02,\n",
            "        8.2549855e-02, -2.6167394e-03,  8.2263947e-02, -6.6168435e-02,\n",
            "        2.9934989e-02,  1.0558230e-01,  6.6348322e-02,  1.9086453e-01,\n",
            "        1.5234245e-01,  3.1261870e-01,  1.0885191e-01, -4.7468059e-02,\n",
            "       -6.6200122e-02,  3.0920875e-01,  9.1310918e-02,  5.9120517e-02],\n",
            "      dtype=float32)]\n",
            "{'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'} []\n",
            "{'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None} []\n",
            "{'name': 'output_layer', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-8.06158185e-01, -1.73933417e-01,  5.36699235e-01,\n",
            "        -9.17812958e-02, -9.96379972e-01,  2.97341019e-01,\n",
            "         6.13215864e-01, -6.28475070e-01, -1.20606608e-04,\n",
            "        -1.23539791e-01],\n",
            "       [ 4.37439948e-01, -9.34214950e-01,  1.78368926e-01,\n",
            "         3.09234768e-01, -1.14283741e+00,  1.69502139e-01,\n",
            "         3.88717324e-01, -9.09977973e-01,  3.29425216e-01,\n",
            "         3.22182089e-01],\n",
            "       [ 3.22949678e-01,  1.42827660e-01, -1.37487873e-01,\n",
            "        -3.24581474e-01,  4.55313884e-02, -1.27128708e+00,\n",
            "        -9.53117967e-01,  5.97732723e-01,  6.41484261e-02,\n",
            "         6.96353376e-01],\n",
            "       [-1.19175196e+00,  7.60156810e-01, -2.60233492e-01,\n",
            "        -2.81025469e-01,  6.81746364e-01,  5.44674575e-01,\n",
            "        -5.28820455e-01, -3.91950727e-01, -1.29576623e+00,\n",
            "        -2.23386854e-01],\n",
            "       [ 1.84845701e-01, -6.52946293e-01,  7.86138475e-02,\n",
            "         4.15614545e-01, -1.82505906e+00,  2.08855480e-01,\n",
            "        -3.20755206e-02, -6.51026726e-01,  4.28663433e-01,\n",
            "         3.09380502e-01],\n",
            "       [-1.20842885e-02, -8.53941858e-01,  1.02258988e-01,\n",
            "         6.59604967e-01, -1.44590807e+00, -5.78754961e-01,\n",
            "        -1.05180550e+00,  3.65912944e-01,  4.35657620e-01,\n",
            "        -8.92381817e-02],\n",
            "       [ 5.26371598e-01, -3.92942339e-01, -7.41532385e-01,\n",
            "        -1.33356646e-01,  5.99868298e-01,  7.55942613e-02,\n",
            "         3.88432026e-01, -9.28954959e-01, -1.63172352e+00,\n",
            "        -6.97577000e-01],\n",
            "       [-1.24010336e+00,  3.53414357e-01,  3.28768820e-01,\n",
            "        -1.93536460e-01,  4.43412736e-02,  6.75209284e-01,\n",
            "         1.12781920e-01, -8.16638350e-01, -5.45411669e-02,\n",
            "        -4.92028445e-01],\n",
            "       [ 2.51119047e-01, -1.88118458e-01, -2.72150517e-01,\n",
            "         7.06621647e-01, -6.79093182e-01, -1.12337619e-01,\n",
            "        -1.26667643e+00,  7.75504351e-01, -1.20064247e+00,\n",
            "         5.17003596e-01],\n",
            "       [-3.68059814e-01,  1.48135588e-01, -1.77630746e+00,\n",
            "        -2.45708689e-01, -2.86308646e-01,  5.39058149e-01,\n",
            "         4.28558290e-01, -1.30413699e+00, -2.71640867e-01,\n",
            "         1.49734214e-01],\n",
            "       [ 4.67372119e-01,  1.32054329e-01,  4.22922164e-01,\n",
            "        -3.18002403e-01, -1.46890944e-02, -1.29477644e+00,\n",
            "         2.80702591e-01, -6.09064281e-01, -2.43942633e-01,\n",
            "        -9.60764110e-01],\n",
            "       [ 6.14967287e-01,  4.63562980e-02, -5.20395637e-01,\n",
            "        -9.49866891e-01,  6.81253791e-01,  1.51853964e-01,\n",
            "        -6.47424102e-01,  5.55401266e-01, -9.91721034e-01,\n",
            "         4.77485418e-01],\n",
            "       [-7.91211367e-01, -5.58230206e-02,  9.01914775e-01,\n",
            "         6.25510454e-01, -7.16916621e-01, -1.49502933e+00,\n",
            "        -8.03432345e-01, -9.38263714e-01, -2.56102264e-01,\n",
            "         1.20957300e-01],\n",
            "       [-1.11626804e+00,  3.07154119e-01, -1.90441832e-01,\n",
            "         2.29562908e-01,  4.18851227e-01, -5.42254686e-01,\n",
            "        -9.75274146e-01,  3.20991755e-01,  1.80382147e-01,\n",
            "         1.60871312e-01],\n",
            "       [-1.36010385e+00, -2.00968329e-02, -1.42428175e-01,\n",
            "         2.00889990e-01, -1.09253728e+00,  2.45921299e-01,\n",
            "        -1.60039616e+00,  3.59552592e-01, -8.56349319e-02,\n",
            "         4.72008377e-01],\n",
            "       [ 1.27461806e-01,  6.90214515e-01, -1.12268841e+00,\n",
            "        -3.48512918e-01, -1.73569173e-01, -3.71509641e-01,\n",
            "        -7.59824216e-01, -7.38601923e-01, -5.02844714e-02,\n",
            "         7.74959683e-01],\n",
            "       [-9.26231623e-01, -7.21971571e-01,  2.87488043e-01,\n",
            "         2.96551615e-01, -1.26636910e+00, -4.23255175e-01,\n",
            "        -1.43344855e+00,  4.90559876e-01,  2.97673762e-01,\n",
            "         2.53205717e-01],\n",
            "       [-1.43310595e-02, -1.51226735e+00, -5.87544501e-01,\n",
            "         4.03942734e-01, -8.59275162e-01,  6.11996949e-01,\n",
            "        -9.43673790e-01,  3.71879846e-01, -9.83599275e-02,\n",
            "         3.46872240e-01],\n",
            "       [ 5.37823498e-01, -2.36237973e-01, -5.67084253e-01,\n",
            "         6.67595029e-01, -3.41043264e-01,  4.42056000e-01,\n",
            "         3.52372885e-01, -1.14396036e+00, -1.30831003e+00,\n",
            "         1.30955979e-01],\n",
            "       [-5.70195854e-01,  3.14572275e-01,  5.95332384e-01,\n",
            "         1.43336337e-02, -6.43683016e-01, -4.47104931e-01,\n",
            "         3.77347410e-01, -9.34652328e-01,  2.01462865e-01,\n",
            "        -1.27846634e+00],\n",
            "       [-7.57091582e-01,  2.62863576e-01,  7.05726385e-01,\n",
            "         3.00634168e-02, -7.79797196e-01,  3.90181273e-01,\n",
            "        -1.04839826e+00,  2.56207079e-01, -5.24597585e-01,\n",
            "        -1.14253473e+00],\n",
            "       [-5.34010410e-01,  2.43245795e-01,  2.92216897e-01,\n",
            "        -3.68250370e-01,  1.92154273e-01, -1.68340123e+00,\n",
            "        -4.67668176e-01, -3.82656083e-02,  3.70399445e-01,\n",
            "         4.48062092e-01],\n",
            "       [-2.42835507e-01,  2.13034645e-01, -1.71165061e+00,\n",
            "         1.54898716e-02,  2.04876348e-01,  4.29949880e-01,\n",
            "        -9.92087245e-01,  4.01807517e-01, -1.67300850e-01,\n",
            "         1.14914589e-01],\n",
            "       [ 1.46759609e-02, -1.03552902e+00, -3.07545632e-01,\n",
            "         7.14021504e-01,  2.95034885e-01,  4.49302554e-01,\n",
            "         3.61673325e-01,  1.04200804e+00, -8.23369920e-01,\n",
            "        -1.29336727e+00],\n",
            "       [ 6.16289318e-01,  2.12691441e-01, -5.07347472e-03,\n",
            "        -1.46932662e+00,  6.71187818e-01, -7.04946160e-01,\n",
            "        -7.63453364e-01,  5.92031538e-01, -3.60338360e-01,\n",
            "         2.52878159e-01],\n",
            "       [-1.07358563e+00,  2.44181722e-01,  4.01100993e-01,\n",
            "         4.66408551e-01, -1.51641917e+00, -1.35795385e-01,\n",
            "        -7.55659699e-01,  2.47191712e-01, -4.96704616e-02,\n",
            "        -1.63890019e-01],\n",
            "       [-5.81821442e-01,  6.93568140e-02, -2.03567863e+00,\n",
            "        -4.39317048e-01,  4.31970567e-01,  4.57267731e-01,\n",
            "         9.03350636e-02,  2.94185877e-01,  5.60598820e-02,\n",
            "         3.47057909e-01],\n",
            "       [-9.40931320e-01,  2.39275426e-01,  6.38521492e-01,\n",
            "        -8.34667087e-02, -4.08125609e-01, -5.04258871e-01,\n",
            "         2.81774789e-01,  4.84843850e-01,  2.03299895e-01,\n",
            "        -1.36571109e+00],\n",
            "       [-5.01470208e-01, -2.93529212e-01, -4.04182196e-01,\n",
            "         4.28169012e-01, -4.22878861e-01, -9.96470571e-01,\n",
            "        -1.20904744e+00,  7.34928310e-01,  4.35102284e-02,\n",
            "         4.45471764e-01],\n",
            "       [-1.67677194e-01,  4.41581875e-01,  1.62542596e-01,\n",
            "        -5.96094966e-01,  3.94844502e-01, -2.00176582e-01,\n",
            "         3.87920231e-01, -1.02410626e+00,  1.62129506e-01,\n",
            "        -1.10980082e+00],\n",
            "       [ 1.66459560e-01, -2.11037114e-01, -8.24491829e-02,\n",
            "         2.12677255e-01, -2.18576097e+00, -3.12215447e-01,\n",
            "        -1.55208096e-01, -7.94869125e-01,  2.31131092e-01,\n",
            "         2.63024956e-01],\n",
            "       [ 2.61266470e-01, -7.76557267e-01, -9.96556699e-01,\n",
            "        -8.71663272e-01, -1.53903589e-01,  2.29566514e-01,\n",
            "         5.03220744e-02, -9.02726257e-04, -1.35518715e-01,\n",
            "        -1.58217609e-01],\n",
            "       [ 3.95797998e-01, -6.68326139e-01, -9.74686027e-01,\n",
            "        -9.40088332e-01,  2.82880902e-01,  1.05610274e-01,\n",
            "         3.88425514e-02,  1.96259871e-01, -3.88393283e-01,\n",
            "         1.78551495e-01],\n",
            "       [-5.93068242e-01,  4.21710223e-01,  4.99582857e-01,\n",
            "         3.85626629e-02,  1.07957996e-01, -1.02065754e+00,\n",
            "         3.73598635e-01, -9.11250710e-01,  3.24660271e-01,\n",
            "        -9.38717186e-01],\n",
            "       [ 2.41776794e-01, -2.53031626e-02, -8.37202072e-01,\n",
            "        -1.36339271e+00,  2.86620170e-01,  3.86040032e-01,\n",
            "         1.99883327e-01,  3.14977050e-01, -2.06172004e-01,\n",
            "        -5.29686153e-01],\n",
            "       [-6.16219223e-01,  5.82062542e-01,  7.58947253e-01,\n",
            "        -3.23753208e-01, -5.14469802e-01,  2.14692026e-01,\n",
            "        -1.23961973e+00,  6.55083418e-01, -2.95622587e-01,\n",
            "        -7.62461245e-01],\n",
            "       [-9.37883973e-01,  8.02092433e-01, -6.12509072e-01,\n",
            "         1.33965492e-01, -1.01414490e+00, -5.53167403e-01,\n",
            "        -9.14919555e-01, -1.65202975e-01, -1.76739585e+00,\n",
            "         9.61996377e-01],\n",
            "       [-6.09755456e-01,  3.89022380e-01,  5.11659741e-01,\n",
            "         1.55998573e-01, -3.17186527e-02, -1.01317835e+00,\n",
            "         3.77333105e-01, -1.04049027e+00,  2.71141231e-01,\n",
            "        -7.18213797e-01],\n",
            "       [ 3.57264131e-01,  4.25384343e-02, -6.88343763e-01,\n",
            "        -3.44512403e-01,  6.41987145e-01, -6.15219891e-01,\n",
            "         4.84053284e-01,  5.65898538e-01, -5.55980325e-01,\n",
            "        -1.05063999e+00],\n",
            "       [ 3.32935691e-01, -8.63922536e-02, -1.23761606e+00,\n",
            "        -6.78920269e-01, -1.68333143e-01,  5.35046577e-01,\n",
            "         3.25491190e-01, -3.85200649e-01,  6.06206596e-01,\n",
            "        -2.24157199e-01],\n",
            "       [-1.28255379e+00, -2.58967817e-01,  4.50698972e-01,\n",
            "         5.23011744e-01, -1.53862119e+00, -7.17236280e-01,\n",
            "        -1.03791499e+00,  3.82145584e-01,  3.43020290e-01,\n",
            "        -1.61603615e-01],\n",
            "       [-8.25172603e-01,  3.52014482e-01, -1.38160658e+00,\n",
            "        -3.90348226e-01,  2.67243594e-01,  3.72580111e-01,\n",
            "        -8.18787158e-01, -6.13103583e-02,  1.34986177e-01,\n",
            "         5.66633716e-02],\n",
            "       [ 4.11071092e-01,  8.91859829e-02,  6.56759590e-02,\n",
            "        -1.29342425e+00,  2.42384732e-01, -9.57893074e-01,\n",
            "         2.54466295e-01,  1.37595341e-01,  2.95057148e-01,\n",
            "        -6.42249943e-04],\n",
            "       [-1.52503574e+00,  4.15419415e-02,  3.37276936e-01,\n",
            "         3.33515912e-01, -1.47981012e+00, -2.39609201e-02,\n",
            "        -1.04903388e+00,  2.91431636e-01,  2.56845534e-01,\n",
            "         8.00283551e-02],\n",
            "       [ 4.93408173e-01,  2.46829465e-02, -5.34928322e-01,\n",
            "        -1.40335238e+00,  4.69600469e-01, -6.40341103e-01,\n",
            "        -1.02771744e-01,  2.56006420e-01, -2.21063480e-01,\n",
            "         3.77011001e-01],\n",
            "       [-5.73617399e-01,  3.51435274e-01, -1.00485218e+00,\n",
            "        -6.42389357e-01,  3.73667151e-01,  1.42755702e-01,\n",
            "         3.59977126e-01, -3.15075278e-01, -1.47601649e-01,\n",
            "        -4.36566889e-01],\n",
            "       [ 4.62932497e-01, -1.62549391e-01,  3.96949440e-01,\n",
            "        -3.75610977e-01,  2.49105588e-01, -9.64915097e-01,\n",
            "         3.90538722e-01, -1.01325190e+00,  3.60261738e-01,\n",
            "        -1.37025416e+00],\n",
            "       [-9.40055966e-01,  3.35494936e-01, -1.41367042e+00,\n",
            "        -3.27621728e-01,  4.31901574e-01, -1.18385188e-01,\n",
            "         4.59753662e-01, -9.83657781e-03,  2.42300212e-01,\n",
            "         2.77407736e-01],\n",
            "       [ 4.70268697e-01,  7.60095417e-01,  2.82715291e-01,\n",
            "         5.95432296e-02,  3.45195979e-01,  4.89436835e-01,\n",
            "        -5.60356081e-01, -4.42793190e-01, -1.04526889e+00,\n",
            "        -2.73202747e-01],\n",
            "       [ 5.77164710e-01,  1.59997657e-01, -9.65541780e-01,\n",
            "        -1.04380453e+00,  4.39614296e-01, -7.70630121e-01,\n",
            "         1.73281804e-01, -9.34217796e-02,  3.87048155e-01,\n",
            "         3.24625105e-01],\n",
            "       [-1.38579607e+00,  8.95686373e-02,  2.33528927e-01,\n",
            "         1.91341311e-01, -1.00606167e+00,  6.45954192e-01,\n",
            "         8.50727558e-02, -7.35910535e-01, -8.98857117e-02,\n",
            "        -5.53336024e-01],\n",
            "       [ 1.81660149e-02,  1.16512038e-01, -1.70798349e+00,\n",
            "        -9.42088068e-01,  2.93190569e-01,  2.20713884e-01,\n",
            "         2.19753370e-01, -5.95716178e-01,  3.23099613e-01,\n",
            "         2.60527402e-01],\n",
            "       [-7.08107948e-01,  3.02534014e-01,  3.47337842e-01,\n",
            "        -9.91168022e-01,  4.07507628e-01, -1.97464064e-01,\n",
            "         8.23429748e-02,  3.61285269e-01,  1.19832508e-01,\n",
            "        -7.72855401e-01],\n",
            "       [-9.35796857e-01,  3.91905233e-02, -1.36007741e-01,\n",
            "         3.74221027e-01, -3.61801177e-01,  4.77965176e-01,\n",
            "         4.53949630e-01, -1.54028988e+00,  1.15103073e-01,\n",
            "        -2.59913117e-01],\n",
            "       [ 4.02571201e-01, -1.01026356e+00, -1.64709997e+00,\n",
            "        -4.17868830e-02, -3.40741366e-01,  4.14379597e-01,\n",
            "         5.51541865e-01,  5.61392382e-02,  2.95954525e-01,\n",
            "         3.55412424e-01],\n",
            "       [-2.47082010e-01, -1.40297961e+00,  1.61378622e-01,\n",
            "         5.00450611e-01, -1.23640633e+00,  3.95430237e-01,\n",
            "        -5.93866885e-01,  1.19369276e-01,  3.90049100e-01,\n",
            "         4.17782515e-01],\n",
            "       [ 4.24253345e-01, -1.21288931e+00,  3.76131713e-01,\n",
            "         2.26760954e-01, -1.48598075e+00, -5.20790398e-01,\n",
            "        -3.92629541e-02, -7.74683893e-01,  4.02118355e-01,\n",
            "         2.76418030e-01],\n",
            "       [ 5.93628466e-01, -8.58441949e-01,  1.05494249e+00,\n",
            "         2.67194778e-01, -4.33326542e-01, -7.35770464e-01,\n",
            "         4.38521028e-01, -1.00793290e+00, -1.13256276e+00,\n",
            "         2.20279291e-01],\n",
            "       [-2.00021282e-01, -1.00330937e+00, -1.29200459e+00,\n",
            "         7.57295370e-01, -6.49837494e-01,  7.31975973e-01,\n",
            "         3.85067552e-01, -4.03698266e-01, -1.57012105e-01,\n",
            "        -1.11421771e-01],\n",
            "       [-2.26749361e-01, -1.26204446e-01, -2.34657899e-01,\n",
            "        -2.64352486e-02, -2.55612463e-01,  1.70161933e-01,\n",
            "        -1.77041993e-01, -1.30709903e-02,  1.43094182e-01,\n",
            "         2.15011671e-01],\n",
            "       [ 2.44908541e-01,  4.28428352e-01, -6.21375084e-01,\n",
            "        -1.04578555e+00,  5.15735030e-01,  2.32682869e-01,\n",
            "         1.60223246e-01, -8.13766420e-01,  3.46856207e-01,\n",
            "        -4.29252654e-01],\n",
            "       [-3.02551568e-01,  2.61437058e-01,  4.60665643e-01,\n",
            "         2.96287715e-01, -2.07207608e+00,  3.53664786e-01,\n",
            "        -4.56565291e-01, -2.65477926e-01,  5.20028695e-02,\n",
            "         1.69790372e-01],\n",
            "       [ 5.67399323e-01,  1.86289459e-01, -1.16281152e-01,\n",
            "        -4.86678630e-01,  5.26150942e-01, -1.16460896e+00,\n",
            "         1.58809230e-01,  1.92733169e-01, -1.19805731e-01,\n",
            "        -6.50450468e-01],\n",
            "       [ 4.58453059e-01, -3.56220126e-01,  2.71249235e-01,\n",
            "         4.45260972e-01, -8.20599437e-01,  1.21935075e-02,\n",
            "        -7.42984265e-02, -1.20358407e+00,  4.88667488e-01,\n",
            "        -2.67565519e-01]], dtype=float32), array([-0.01420109, -0.00143794,  0.0626498 ,  0.13776255, -0.14463067,\n",
            "       -0.03634392,  0.04640959,  0.02704564, -0.10599674, -0.01319453],\n",
            "      dtype=float32)]\n",
            "{'name': 'activation_2', 'trainable': True, 'dtype': 'float32', 'activation': 'softmax'} []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Retrieving Specific Weights</h1>**\n",
        "\n",
        "<li>The layers of the model are stored in a list of length <code>8</code>,   </li>"
      ],
      "metadata": {
        "id": "71Z-jZRXCfxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cjUr-n-D_GY",
        "outputId": "319aad89-10eb-47c8-f0a2-10d55cdc52af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<li>For reference, again, this is the code to define the model. We can see which indexes of the list we need to retrieve the weights are from, those being the layers in which the input, hidden, and output layers are defined, being indexes 1, 4, and 7 respectively.</li>\n",
        "\n",
        "```\n",
        "# define model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, input_shape=(X_train.shape[1],), name=\"input_layer\"))  # layer 1\n",
        "model.add(tf.keras.layers.Activation('relu'))  # layer 2\n",
        "model.add(tf.keras.layers.Dropout(0.2))  # layer 3\n",
        "model.add(tf.keras.layers.Dense(64, name=\"hidden_layer\"))  # layer 4\n",
        "model.add(tf.keras.layers.Activation('relu'))   # layer 5\n",
        "model.add(tf.keras.layers.Dropout(0.2))    # layer 6\n",
        "model.add(tf.keras.layers.Dense(10, name=\"output_layer\"))    # layer 7\n",
        "model.add(tf.keras.layers.Activation('softmax'))    # layer 8\n",
        "```"
      ],
      "metadata": {
        "id": "Mh0YWOJ8FTLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<li>Thus, we can get the weights and biases of each layer, as shown in the following section. Note, the weights of each layer are stored in the first index, while the biases of each layer are stored in the index immediately following it.</li>"
      ],
      "metadata": {
        "id": "hIwOof9xG-FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [0]=weights [1]=biases\n",
        "input_layer_weights = model.layers[0].get_weights()[0]\n",
        "input_layer_biases  = model.layers[0].get_weights()[1]\n",
        "hidden_layer_weights = model.layers[3].get_weights()[0]\n",
        "hidden_layer_biases  = model.layers[3].get_weights()[1]\n",
        "output_layer_weights = model.layers[6].get_weights()[0]\n",
        "output_layer_biases  = model.layers[6].get_weights()[1]"
      ],
      "metadata": {
        "id": "RqnKs3vd_5yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Defining Functions Necessary for Forward Propogation</h1>**\n",
        "\n",
        "<li>Below, we define functions necessary for forward propogation. These are all given in the Programming Exercises problems 1 to 6, and 8.</li>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "upJO2tOkq8tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# initialize weights\n",
        "def init_weights(n_inputs, n_hidden, n_output):\n",
        "  W0 = np.random.randn(n_inputs, n_hidden)\n",
        "  W1 = np.random.randn(n_hidden, n_hidden)\n",
        "  W2 = np.random.randn(n_hidden, n_output)\n",
        "  return W0, W1, W2\n",
        "\n",
        "# feedforward function\n",
        "def feedforward(x, W0, W1, W2):\n",
        "  z0 = np.dot(x, W0)\n",
        "  a0 = sigmoid(z0)\n",
        "  z1 = np.dot(a0, W1)\n",
        "  a1 = sigmoid(z1)\n",
        "  z2 = np.dot(a1, W2)\n",
        "  a2 = sigmoid(z2)\n",
        "  return z0, a0, z1, a1, z2, a2\n",
        "\n",
        "# predict function\n",
        "def predict(x, W0, W1, W2):\n",
        "  _,_, _, _, _, a2 = feedforward(x, W0, W1, W2)\n",
        "  return a2\n",
        "\n",
        "# backpropogation function\n",
        "def backprop(X_training, Y_training, W0, W1, W2, learning_rate):\n",
        "  m = len(X_training)\n",
        "  for x, y in zip(X_training, Y_training):\n",
        "    z0, a0, z1, a1, z2, a2 = feedforward(x, W0, W1, W2)\n",
        "    dz2 = a2 - y\n",
        "    dw2 = np.dot(a1.T, dz2) / m\n",
        "    dz1 = np.dot(dz2, W2.T) * sigmoid(z1) * (1 - sigmoid(z1))\n",
        "    dw1 = np.dot(a0.T, dz1) / m\n",
        "    dz0 = np.dot(dz1, W1.T) * sigmoid(z0) * (1 - sigmoid(z0))\n",
        "    dw0 = np.dot(x.T, dz0) / m\n",
        "    W0 -= learning_rate * dw0\n",
        "    W1 -= learning_rate * dw1\n",
        "    W2 -= learning_rate * dw2\n",
        "  return W0, W1, W2\n",
        "\n",
        "#loss function\n",
        "def loss(Y_pred, Y):\n",
        "  return np.mean((Y_pred - Y) ** 2)\n",
        "\n",
        "#main function\n",
        "def main(X_training, Y_training, n_inputs, n_hidden, n_output, n_epochs,\n",
        "learning_rate):\n",
        "  W0, W1, W2 = init_weights(n_inputs, n_hidden, n_output)\n",
        "  for epoch in range(n_epochs):\n",
        "    W0, W1, W2 = backprop(X_training, Y_training, W0, W1, W2, learning_rate)\n",
        "    Y_pred = predict(X_training, W0, W1, W2)\n",
        "    train_loss = loss(Y_pred, Y_training)\n",
        "    print(epoch, train_loss)\n",
        "  return W0, W1, W2, train_loss\n",
        "\n",
        "# define the linear function\n",
        "def linear_function(W, A_prev):\n",
        "  Z = np.dot(W.T, A_prev)\n",
        "  return Z\n",
        "\n",
        "# define the non-linear function\n",
        "def non_linear_function(Z, activation_function):\n",
        "  A = activation_function(Z)\n",
        "  return A\n",
        "\n",
        "# define the forward propagation function\n",
        "def forward_propagation(W, A_prev, activation_function):\n",
        "  Z = linear_function(W, A_prev)\n",
        "  A = non_linear_function(Z, activation_function)\n",
        "  return A\n",
        "\n",
        "# relu nonlinear activation function\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "# softmax nonlinear activation function\n",
        "def softmax(z):\n",
        "  exp_z = np.exp(z)\n",
        "  return exp_z / exp_z.sum()\n"
      ],
      "metadata": {
        "id": "En6R3T6Hw3vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Defining NumPy Network (Nonfunctional)</h1>**\n",
        "\n",
        "<li>Below, we attempt to create the neural network through NumPy.</li>\n"
      ],
      "metadata": {
        "id": "AhQ2cwMNDRpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set training data\n",
        "x_training = X_train\n",
        "y_training = Y_train\n",
        "\n",
        "# Initialize network\n",
        "n_inputs = 64\n",
        "n_hidden = 64\n",
        "n_output = 10\n",
        "\n",
        "W0, W1, W2 = init_weights(n_inputs, n_hidden, n_output)\n",
        "\n",
        "# Train the network\n",
        "W0, W1, W2 = main(x_training, y_training, n_inputs, n_hidden, n_output, n_epochs=400, learning_rate=0.2)\n",
        "\n",
        "# Test the network\n",
        "x_test = np.random.randn(1, 10)\n",
        "y_pred = predict(x_test, W0, W1, W2)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "lSGekgbr_08S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Execute Forward Propogation (Nonfunctional)</h1>**\n",
        "\n",
        "<li>Below, we attempt to execute forward propogation of the NumPy network.</li>\n"
      ],
      "metadata": {
        "id": "jEQWQ95rDub6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "W = np.array(input_layer_weights)\n",
        "A_prev = np.array([0])\n",
        "b = np.array(input_layer_biases)\n",
        "\n",
        "# Append bias vector to W\n",
        "W = np.append(W, b.reshape(-1,1), axis=1)\n",
        "\n",
        "# Append bias term to A_prev\n",
        "A_prev = np.append(A_prev, hidden_layer_biases)\n",
        "\n",
        "# Perform forward propagation\n",
        "A = forward_propagation(W, A_prev, relu)\n",
        "A = forward_propagation(W, A_prev, relu)\n",
        "A = forward_propagation(W, A_prev, softmax)\n",
        "# Output: A = sigmoid(W.T * A_prev)"
      ],
      "metadata": {
        "id": "jB4V8kl6DPWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Task 3.</h1>**\n"
      ],
      "metadata": {
        "id": "0Eki1KPsnD2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Verify Accuracy</h1>**\n",
        "\n",
        "<li>Below, we plot images for several results in each class.</li>\n",
        "\n",
        "\n",
        "> <li>The chart below displays the model accuracy and loss over time while training.</li>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0bvkaRpnQ4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the metrics for model\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.title('Model Accuracy') \n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy') \n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy']) \n",
        "\n",
        "plt.legend(['Train', 'Test'], loc='lower right')\n",
        "\n",
        "plt.subplot(2,1,2) \n",
        "plt.title('Model Loss') \n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss') \n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "\n",
        "plt.legend(['Train', 'Test'], loc='upper right') \n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "6BGp-MK8zGwE",
        "outputId": "b14a9ee9-ee20-41ad-8137-0c695deeae70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><li>For reference, this is the training loss and training accuracy of the model.</li>\n"
      ],
      "metadata": {
        "id": "ZartGvF8jJdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print(\"train_loss:\", test_loss)\n",
        "print(\"train_acc:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gndP63TMC3fb",
        "outputId": "b09fa322-ac24-4741-a986-d2a026555132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9600\n",
            "train_loss: 0.19132837653160095\n",
            "train_acc: 0.9599555134773254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Model Predictions</h1>**\n",
        "\n",
        "<li>Below, we generate predictions with our model on the test split of the dataset.</li>\n",
        "\n",
        "> <li>We append predictions that match the expected value in the <code>correct_indices</code> list, append predictions that do not match the expected value in the <code>incorrect_indices</code> list.</li>\n"
      ],
      "metadata": {
        "id": "jXWKVRgwjXjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions with model on test set\n",
        "predictions = (model.predict(X_test) > 0.1).astype(\"float32\")\n",
        "\n",
        "correct_indices = np.nonzero(predictions == Y_test)[0] \n",
        "incorrect_indices = np.nonzero(predictions != Y_test)[0]\n",
        "\n",
        "print(len(correct_indices),\" classified correctly\") \n",
        "print(len(incorrect_indices),\" classified incorrectly\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4t4Pvi2KIo2",
        "outputId": "cc7ff57f-04a7-47d8-8e66-baa3910353d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 0s 2ms/step\n",
            "8882  classified correctly\n",
            "108  classified incorrectly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Visualizing Predictions</h1>**\n",
        "\n",
        "<li>Below, we output the first 25 indexes of the <code>correct_indices</code> list, outputting both the predicted value and the truth value.</li>\n",
        "\n",
        "> <li>We can see below that a predicted value of <code>[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]</code> corresponds to the outputted chart of a figure resembling an <code>6</code>, and that a predicted value of <code>[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]</code> corresponds to the outputted chart of a figure resembling an <code>7</code>.</li>"
      ],
      "metadata": {
        "id": "E9dX6npTnv0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot correct predicted values and graphs\n",
        "for i, correct in enumerate(correct_indices[:25]): \n",
        "  img = X_test[correct].reshape(8,8)\n",
        "  plt.subplot(5,5,i+1) \n",
        "  plt.imshow(img, cmap=\"Greys\")\n",
        "  print(\"Predicted: {}, Truth: {}\".format(predictions[correct],Y_test[correct]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "1tJsRHG3LhyG",
        "outputId": "acc7b899-9fc0-4a9c-a6f4-184bd435d788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAD6CAYAAAD6Bm+iAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWKklEQVR4nO3dQYhcVb7H8d//9WMckMA41MRF1JRBbXwbwTRNFiJkkaALURczaHbZuDCCgwgKk90YcKPMxllk0bORHplBEt5A0IQsFFx1NbzHM/KMSdPR9CYpJgFF5E3LeQur2676n1TdW31u1T23vh+QpM7cvnXzZe7hcqv6HgshCACQzr9N+wAAoGmYWAEgMSZWAEiMiRUAEmNiBYDEmFgBILFCE6uZPWVmX5rZFTN7s+qDygFNPJrE0cVrehMb9T1WM5uTdFnSEUnXJa1IejGE8MWdfqbVaoV2uz10v99//70b+/rrr93Y/Px87JiG7jtmfX1d3W63/A9G0MRrShNJWl1d7YYQfjPWD/tjKNWFJnF17DLs/Pn3Aj+/KOlKCGGtdwAfSHpW0h0jtNttdTqdoTuN/e+vvvqqG7t48aIb++UvfznikL2FhYXSPzMETbxGNJEkM7s21g/GlepCk7g6dhl2/hS5FbBP0jc7Xl/vjc0ymng0iaOL1/gmyT68MrOXzKxjZp2bN2+m2m3WaOLRxKNJXM5dikysG5Lu3/H6vt5YnxDC6RDCQghh4Te/SXIrps5o4tEkbmQXmjTv/ytF7rGuSHrYzB7UT//4FyQd2+0bnzp1qtB2494TqhhNPJrEJe9Ck7g6dRk5sYYQNs3sFUkfS5qTtBRCuFT5kdUYTTyaxNHFm4UmRa5YFUI4J+lcxceSFZp4NImji9f0JvzmFQAkVuiKtQpnz551Y++9997kD6RGaOLRxKNJXJ26cMUKAIkxsQJAYkysAJAYEysAJDaxD69GPUBBko4d2/V3hLNCE48mHk3i6tyFK1YASIyJFQASY2IFgMSYWAEgsYl9eLVnz56R2ywvL7uxkydPurHXXnut7/Xrr7/utqnxk3220cSjiUeTuDp34YoVABJjYgWAxJhYASCxQvdYzWxd0reSfpS0GUJIurxnjmgSRxePJl7Tm5T58OpwCKFb2ZFIOnHihBt77rnn3Ni7777b93ptbc1ts7S0lOy4hqBJXKVdaOLRJG5aXbgVAACJFZ1Yg6TzZrZqZi/FNsh5qdox0SRuaBea0KSn0edP0Yn1iRDC45KelnTCzJ4c3CDnpWrHRJO4oV1oQpOeRp8/RRcT3Oj9ecPMzkhalPRpmTfav39/3+t77rnHbdNqtdzY22+/7caeeeaZMm9dCZrE7bYLTTyaxNW5y8grVjO728z2bP1d0lFJnyc9iszQJI4uHk28WWhS5Ir1XklnzGxr++UQwkeVHlX90SSOLh5NvMY3GTmxhhDWJD02gWPJBk3i6OLRxJuFJnzdCgASm9jTrQafDHP+/Hm3TWwZhdhN5cGxU6dO7fLopoMmHk08msTVuQtXrACQGBMrACTGxAoAiTGxAkBiFkJIv1Ozm5KuSWpJqurpNWX3vT+EMLXfi5tQE5XcP03iptZlRxOJ82dbbnNKJRPr9s7NOlU9Z7HKfVep6uPOsQtN4jh/vFyacCsAABJjYgWAxKqeWE9nuu8qVX3cOXahSRznj5dFk0rvsQLALCp0xWpmT5nZl2Z2xczerPqgckATjyZxdPGa3mTkFauZzUm6LOmIpOuSViS9GEL44k4/02q1QrvdHrrf77//3o19/fXXbmx+fj52TEP3HbO+vq5ut1v+ByNo4jWliSStrq52U321qGwXmsTVscuw86fIQ1gWJV3pPepLZvaBpGclRSOY2dzBgwfV6XSG7jT2v7/66qtu7OLFi25s8OELRSwsJP2GRqkmktRut2kyoI5NJMnMro3eqrBSXWgSfe/s5pQitwL2Sfpmx+vrvbE7WSx2WFkr22QW0CSOLl7j55Rk3wrYWlFR0vu5rahYlZxXmawKTTyaxOU8pxSZWDck3b/j9X29sT5bKypKeiO3FRXHUKpJjqtMjoEmcSO70KR5c0qRe6wrkh42swf10z/+BUn+6bE/c4Fiij5Idtx7QhUr26QQmniZN5Eq6DKDTbKbU4qsebVpZq9I+ljSnKSlEMKlIT+ykurg6mqMJo1Hkzi6eLMwpxRamiWEcE7SuYLbbib+tLmWyjSZFTSJo4vX9DmFZwUAQGITW0xw0NmzZ93Ye++9N/kDqRGaeDTxaBJXpy5csQJAYkysAJAYEysAJMbECgCJTezDq1EPUJCkY8d2/X3yrNDEo4lHk7g6d+GKFQASY2IFgMSYWAEgMSZWAEhsYh9e7dmzZ+Q2y8vLbuzkyZNu7LXXXut7/frrr7ttavxkn2008Wji0SSuzl24YgWAxJhYASAxJlYASKzQPVYzW5f0raQfJW32lkuYaTSJo4tHE6/pTcp8eHU4hNCt7EgknThxwo0999xzbuzdd9/te722tua2WVpaSnZcQ9AkrtIuNPFoEjetLtwKAIDEik6sQdJ5M1s1s5diG8zgEr40iRvahSY06Wn0+VN0Yn0ihPC4pKclnTCzJwc3mMElfGkSN7QLTWjS0+jzp+highu9P2+Y2RlJi5I+LfNG+/fv73t9zz33uG1arZYbe/vtt93YM888U+atK0GTuN12oYlHk7g6dxl5xWpmd5vZnq2/Szoq6fOkR5EZmsTRxaOJNwtNilyx3ivpjJltbb8cQvio0qOqP5rE0cWjidf4JiMn1hDCmqTHJnAs2aBJHF08mniz0ISvWwFAYhN7utXgk2HOnz/vtoktoxC7qTw4durUqV0e3XTQxKOJR5O4OnfhihUAEmNiBYDEmFgBIDEmVgBIzEII6XdqdlPSNUktSVU9vabsvveHEKb2e3ETaqKS+6dJ3NS67Ggicf5sy21OqWRi3d65Waeq5yxWue8qVX3cOXahSRznj5dLE24FAEBiTKwAkFjVE+vpTPddpaqPO8cuNInj/PGyaFLpPVYAmEWFrljN7Ckz+9LMrpjZm1UfVA5o4tEkji5e05uMvGI1szlJlyUdkXRd0oqkF0MIX9zpZ1qtVmi320P3e/XqVTd2+/ZtN3bXXXe5sYceeqjv9eDvDMesr6+r2+3ayA0LoInXlCaStLq62k311aKyXWgSV8cuw86fIg9hWZR0pfeoL5nZB5KelRSNYGZzBw8eVKfTGbrT559/3o2dPXvWjT3wwANu7MMPP+x7PT8/P/S9JGlhIek3NEo1kaR2u02TAXVsIklmdm30VoWV6kKT6HtnN6cUuRWwT9I3O15f743dyWKBfeaubJNZQJM4uniNn1OSfStga0VFSe/ntqJiVXJeZbIqNPFoEpfznFJkYt2QdP+O1/f1xvpsrago6Y3cVlQcQ6kmOa4yOQaaxI3sQpPmzSlF7rGuSHrYzB7UT//4FyT5p8f+zAWKOXLkiBu7dOmSG/vqq6/c2OnT/V83e+edd4q8ZUplmxRCEy/zJlIFXWawSXZzSpE1rzbN7BVJH0uak7QUQvBH+7OVXR1RBsZo0ng0iaOLNwtzSqGlWUII5ySdK7jtZuJPm2upTJNZQZM4unhNn1N4VgAAJFbJr7QuLCyEUd85K+qRRx5xY4M3sj/77LMix6ROp5Pky/DjoEn0/WvXRJLMbHVaj9SjSVwduww7f7hiBYDEmFgBIDEmVgBIjIkVABIr9HWraYr9xsWhQ4emcCT1QROPJh5N4ibRhStWAEiMiRUAEmNiBYDEmFgBILGJfXg1uETCgQMH3Da3bt0aa98bG/7hN/v21f9ZwjTxaOLRJK7OXbhiBYDEmFgBIDEmVgBIrNA9VjNbl/StpB8lbU7zKTd1QZM4ung08ZrepMyHV4dDCN1x32hwne633nrLbXPhwgU39sknn7ixvXv39r3+7rvvxj2s3aJJ3NhdaOLRJK7OXbgVAACJFZ1Yg6TzZrZqZi/FNpjBJXxpEje0C01o0tPo86foxPpECOFxSU9LOmFmTw5uMINL+NIkbmgXmtCkp9HnT9HFBDd6f94wszOSFiV9WuaNBu+HHDvmV7s9efKkG4vdN3n55ZfLvHUlaBK32y408WgSV+cuI69YzexuM9uz9XdJRyV9nvQoMkOTOLp4NPFmoUmRK9Z7JZ0xs63tl0MIH1V6VPVHkzi6eDTxGt9k5MQaQliT9NgEjiUbNImji0cTbxaa8HUrAEhsakuzHD9+3I09+uijbqwuN9sngSYeTTyaxNWpC1esAJAYEysAJMbECgCJMbECQGIWQki/U7Obkq5Jakka++k1I5Td9/4QwtR+L25CTVRy/zSJm1qXHU0kzp9tuc0plUys2zs361T1nMUq912lqo87xy40ieP88XJpwq0AAEiMiRUAEqt6Yj2d6b6rVPVx59iFJnGcP14WTSq9xwoAs6jQFauZPWVmX5rZFTN7s+qDygFNPJrE0cVrepORV6xmNifpsqQjkq5LWpH0Ygjhizv9TKvVCu12e+h+r1696sZu377txu666y439tBDD/W9Hnzgbcz6+rq63a6N3LAAmnhNaSJJq6ur3VRfLSrbhSZxdewy7Pwp8hCWRUlXeo/6kpl9IOlZSdEIZjZ38OBBdTqdoTt9/vnn3djZs2fd2AMPPODGPvzww77X8/PzQ99LkhYWkn5Do1QTSWq32zQZUMcmkmRm10ZvVVipLjSJvnd2c0qRWwH7JH2z4/X13tidLBbYZ+7KNpkFNImji9f4OSXZtwK2VlSU9H5uKypWJedVJqtCE48mcTnPKUUm1g1J9+94fV9vrM/WioqS3shtRcUxlGqS4yqTY6BJ3MguNGnenFLkHuuKpIfN7EH99I9/QZJfDvFnLlDMkSNH3NilS5fc2FdffeXGTp/u/7rZO++8U+QtUyrbpBCaeJk3kSroMoNNsptTiqx5tWlmr0j6WNKcpKUQgj/an63s6ogyMEaTxqNJHF28WZhTCi3NEkI4J+lcwW03E3/aXEtlmswKmsTRxWv6nMKzAgAgsUp+pXVhYSGM+s5ZUY888ogbG7yR/dlnnxU5JnU6nSRfhh8HTaLvX7smkmRmq9N6pB5N4urYZdj5wxUrACTGxAoAiTGxAkBiTKwAkFihr1tNU+w3Lg4dOjSFI6kPmng08WgSN4kuXLECQGJMrACQGBMrACTGxAoAiU3sw6vBJRIOHDjgtrl169ZY+97Y8A+/2bev/s8SpolHE48mcXXuwhUrACTGxAoAiTGxAkBihe6xmtm6pG8l/Shpc5pPuakLmsTRxaOJ1/QmZT68OhxC6I77RoPrdL/11ltumwsXLrixTz75xI3t3bu37/V333037mHtFk3ixu5CE48mcXXuwq0AAEis6MQaJJ03s1Uzeym2wQwu4UuTuKFdaEKTnkafP0Un1idCCI9LelrSCTN7cnCDGVzClyZxQ7vQhCY9jT5/ii4muNH784aZnZG0KOnTMm80eD/k2DG/2u3JkyfdWOy+ycsvv1zmrStBk7jddqGJR5O4OncZecVqZneb2Z6tv0s6KunzpEeRGZrE0cWjiTcLTYpcsd4r6YyZbW2/HEL4qNKjqj+axNHFo4nX+CYjJ9YQwpqkxyZwLNmgSRxdPJp4s9CEr1sBQGJTW5rl+PHjbuzRRx91Y3W52T4JNPFo4tEkrk5duGIFgMSYWAEgMSZWAEiMiRUAErMQQvqdmt2UdE1SS9LYT68Zoey+94cQpvZ7cRNqopL7p0nc1LrsaCJx/mzLbU6pZGLd3rlZp6rnLFa57ypVfdw5dqFJHOePl0sTbgUAQGJMrACQWNUT6+lM912lqo87xy40ieP88bJoUuk9VgCYRYWuWM3sKTP70syumNmbVR9UDmji0SSOLl7Tm4y8YjWzOUmXJR2RdF3SiqQXQwhf3OlnWq1WaLfbpQ/m1q1bbuyf//ynG/vXv/7V93p+ft5t03sk2bb19XV1u11zG46BJl5TmkjS6upqN9VXi8p2oUlcHbsMO3+KPIRlUdKV3qO+ZGYfSHpWUjSCmc0dPHhQnU6nwK77/f3vf3djy8vLbuzGjRt9ry9evOi2GXy6+MJC0m9olGoiSe12myYD6thEkszsmhscX6kuNPFynFOK3ArYJ+mbHa+v98buZLHAPnNXtsksoEkcXbzGzynJvhWwtaKipPdzW1GxKjmvMlkVmng0ict5TikysW5Iun/H6/t6Y322VlSU9EZuKyqOoVSTHFeZHANN4kZ2oUnz5pQi91hXJD1sZg/qp3/8C5L8cog/c4Fi/vznP7ux2D2Uv/71r27sD3/4Q9/rpaUlt03FD7Mt26QQmniZN5Eq6DKDTbKbU4qsebVpZq9I+ljSnKSlEMKlIT+yUvjdMzVGk8ajSRxdvFmYUwotzRJCOCfpXMFtNxN/2lxLZZrMCprE0cVr+pzCswIAILGpLSZ44cIFN/aXv/yl0M8Ofuk3txvbd0ITjyYeTeLq1IUrVgBIjIkVABJjYgWAxJhYASCxqX14deDAATf2xz/+0Y394x//cGO///3v+17/9re/TXZc00QTjyYeTeLq1IUrVgBIjIkVABJjYgWAxJhYASCxiX149cMPP/S9PnTokNvmd7/7nRuL3XyewNN3JoImHk08msTVuQtXrACQGBMrACTGxAoAiRW6x2pm65K+lfSjpM3ecgkzjSZxdPFo4jW9SZkPrw6HELrjvtHgbzvEbir/7W9/c2ODSyZI0vHjx/te79s3tUUvaRI3dheaeDSJq3MXbgUAQGJFJ9Yg6byZrZrZS7ENZnAJX5rEDe1CE5r0NPr8KTqxPhFCeFzS05JOmNmTgxvM4BK+NIkb2oUmNOlp9PlTdDHBjd6fN8zsjKRFSZ+WeaPBp8XE7n386U9/cmPLy8tu7PDhw32vL1++XOZQkqBJ3G670MSjSVydu4y8YjWzu81sz9bfJR2V9Pmu3jVzNImji0cTbxaaFLlivVfSGTPb2n45hPBRpUdVfzSJo4tHE6/xTUZOrCGENUmPTeBYskGTOLp4NPFmoQlftwKAxKa2NEts6YNf//rXbuzo0aNurNVq9b2+ffu22+ZXv/rV2Mc2LTTxaOLRJK5OXbhiBYDEmFgBIDEmVgBIjIkVABKzEEL6nZrdlHRNUkvS2E+vGaHsvveHEKb2e3ETaqKS+6dJ3NS67Ggicf5sy21OqWRi3d65Waeq5yxWue8qVX3cOXahSRznj5dLE24FAEBiTKwAkFjVE+vpTPddpaqPO8cuNInj/PGyaFLpPVYAmEXcCgCAxCqZWM3sKTP70syumNmbFex/3cz+x8z+y8w6qfdflSq70OSO+8+uC0287JqEEJL+J2lO0lVJByT9QtJ/S/qPxO+xLqmV+tir/K/qLjRpRheaNKNJFVesi5KuhBDWQgj/J+kDSc9W8D65oYtHE48mXnZNqphY90n6Zsfr672xlEau8FhDVXehSVxuXWjiZddkas9j3aUnQggbZrZX0gUz+98QQqmFyBqIJnF08WjiJW1SxRXrhqT7d7y+rzeWTNixwqOkrRUe667SLjSJy7ALTbzsmlQxsa5IetjMHjSzX0h6QdJ/ptp5xis8VtaFJnGZdqGJl12T5LcCQgibZvaKpI/106d5SyGESwnfIssVHivuQpO47LrQxMuxCb95BQCJ8ZtXAJAYEysAJMbECgCJMbECQGJMrACQGBMrACTGxAoAiTGxAkBi/w8np5Rlk3EUZgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<li>Below, we examine the first 25 indexes of the <code>incorrect_indices</code> list, outputting both the predicted value and the truth value. As we can see, the model failed to categorize the figures as they are somewhat ambiguous.</li>\n",
        "\n",
        "> <li>Take for example the second figure, we can see that the model predicted a value of <code>[0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]</code> categorizing the somewhat ambiguous figure as both a <code>8</code>, and a <code>9</code>, when in actuality the expected value was <code>[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]</code>, a figure resembling the digit <code>4</code>.</li>"
      ],
      "metadata": {
        "id": "7pqXSe6dplI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot incorrect predicted values and graphs\n",
        "for i, incorrect in enumerate(incorrect_indices[:25]):\n",
        "  img = X_test[incorrect].reshape(8,8)\n",
        "  plt.subplot(5,5,i+1) \n",
        "  plt.imshow(img, cmap=\"Greys\")\n",
        "  print( \"Predicted {}, Truth: {}\".format(predictions[incorrect], Y_test[incorrect]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "se9wIU24ObCK",
        "outputId": "74b9bd5f-2ba1-42aa-ed21-3b2a3688cc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.], Truth: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.], Truth: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.], Truth: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Predicted [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.], Truth: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Predicted [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.], Truth: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Predicted [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.], Truth: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Conclusions</h1>**\n",
        "\n",
        "<li>We can see that model fitted fairly well and was able to generalize to the test data split.</li>\n",
        "\n",
        "\n",
        "><li>From the predictions examined, the model was able to reasonably recognize digits outside of some fairly ambiguous instances where even a human may not be able to distinguish the digit.</li>\n"
      ],
      "metadata": {
        "id": "RnkAsKVKrgUn"
      }
    }
  ]
}